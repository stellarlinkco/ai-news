{
  "generated_at": "2026-02-22T02:53:37.090034+00:00",
  "sources_checked": 19,
  "new_items_count": 293,
  "items": [
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Our First Proof submissions",
      "url": "https://openai.com/index/first-proof-submissions",
      "summary": "We share our AI model’s proof attempts for the First Proof math challenge, testing research-grade reasoning on expert-level problems.",
      "published_at": "Fri, 20 Feb 2026 14:30:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Advancing independent research on AI alignment",
      "url": "https://openai.com/index/advancing-independent-research-ai-alignment",
      "summary": "OpenAI commits $7.5M to The Alignment Project to fund independent AI alignment research, strengthening global efforts to address AGI safety and security risks.",
      "published_at": "Thu, 19 Feb 2026 10:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Introducing OpenAI for India",
      "url": "https://openai.com/index/openai-for-india",
      "summary": "OpenAI for India expands AI access across the country—building local infrastructure, powering enterprises, and advancing workforce skills.",
      "published_at": "Wed, 18 Feb 2026 21:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "GPT-5.2 derives a new result in theoretical physics",
      "url": "https://openai.com/index/new-result-theoretical-physics",
      "summary": "A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.",
      "published_at": "Fri, 13 Feb 2026 11:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT",
      "url": "https://openai.com/index/introducing-lockdown-mode-and-elevated-risk-labels-in-chatgpt",
      "summary": "Introducing Lockdown Mode and Elevated Risk labels in ChatGPT to help organizations defend against prompt injection and AI-driven data exfiltration.",
      "published_at": "Fri, 13 Feb 2026 10:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Scaling social science research",
      "url": "https://openai.com/index/scaling-social-science-research",
      "summary": "GABRIEL is a new open-source toolkit from OpenAI that uses GPT to turn qualitative text and images into quantitative data, helping social scientists analyze research at scale.",
      "published_at": "Fri, 13 Feb 2026 09:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Beyond rate limits: scaling access to Codex and Sora",
      "url": "https://openai.com/index/beyond-rate-limits",
      "summary": "How OpenAI built a real-time access system combining rate limits, usage tracking, and credits to power continuous access to Sora and Codex.",
      "published_at": "Fri, 13 Feb 2026 09:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Introducing GPT-5.3-Codex-Spark",
      "url": "https://openai.com/index/introducing-gpt-5-3-codex-spark",
      "summary": "Introducing GPT-5.3-Codex-Spark—our first real-time coding model. 15x faster generation, 128k context, now in research preview for ChatGPT Pro users.",
      "published_at": "Thu, 12 Feb 2026 10:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Harness engineering: leveraging Codex in an agent-first world",
      "url": "https://openai.com/index/harness-engineering",
      "summary": "By Ryan Lopopolo, Member of the Technical Staff",
      "published_at": "Wed, 11 Feb 2026 09:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Bringing ChatGPT to GenAI.mil",
      "url": "https://openai.com/index/bringing-chatgpt-to-genaimil",
      "summary": "OpenAI for Government announces the deployment of a custom ChatGPT on GenAI.mil, bringing secure, safety-forward AI to U.S. defense teams.",
      "published_at": "Mon, 09 Feb 2026 11:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Testing ads in ChatGPT",
      "url": "https://openai.com/index/testing-ads-in-chatgpt",
      "summary": "OpenAI begins testing ads in ChatGPT to support free access, with clear labeling, answer independence, strong privacy protections, and user control.",
      "published_at": "Mon, 09 Feb 2026 11:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Making AI work for everyone, everywhere: our approach to localization",
      "url": "https://openai.com/index/our-approach-to-localization",
      "summary": "OpenAI shares its approach to AI localization, showing how globally shared frontier models can be adapted to local languages, laws, and cultures without compromising safety.",
      "published_at": "Fri, 06 Feb 2026 10:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "GPT-5 lowers the cost of cell-free protein synthesis",
      "url": "https://openai.com/index/gpt-5-lowers-protein-synthesis-cost",
      "summary": "An autonomous lab combining OpenAI’s GPT-5 with Ginkgo Bioworks’ cloud automation cut cell-free protein synthesis costs by 40% through closed-loop experimentation.",
      "published_at": "Thu, 05 Feb 2026 11:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Introducing Trusted Access for Cyber",
      "url": "https://openai.com/index/trusted-access-for-cyber",
      "summary": "OpenAI introduces Trusted Access for Cyber, a trust-based framework that expands access to frontier cyber capabilities while strengthening safeguards against misuse.",
      "published_at": "Thu, 05 Feb 2026 10:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Introducing OpenAI Frontier",
      "url": "https://openai.com/index/introducing-openai-frontier",
      "summary": "OpenAI Frontier is an enterprise platform for building, deploying, and managing AI agents with shared context, onboarding, permissions, and governance.",
      "published_at": "Thu, 05 Feb 2026 06:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "GPT-5.3-Codex System Card",
      "url": "https://openai.com/index/gpt-5-3-codex-system-card",
      "summary": "GPT‑5.3-Codex is the most capable agentic coding model to date, combining the frontier coding performance of GPT‑5.2-Codex with the reasoning and professional knowledge capabilities of GPT‑5.2.",
      "published_at": "Thu, 05 Feb 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Navigating health questions with ChatGPT",
      "url": "https://openai.com/index/navigating-health-questions",
      "summary": "A family shares how ChatGPT helped them prepare for critical cancer treatment decisions for their son alongside expert guidance from his doctors.",
      "published_at": "Thu, 05 Feb 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Introducing GPT-5.3-Codex",
      "url": "https://openai.com/index/introducing-gpt-5-3-codex",
      "summary": "GPT-5.3-Codex is a Codex-native agent that pairs frontier coding performance with general reasoning to support long-horizon, real-world technical work.",
      "published_at": "Thu, 05 Feb 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "Unlocking the Codex harness: how we built the App Server",
      "url": "https://openai.com/index/unlocking-the-codex-harness",
      "summary": "Learn how to embed the Codex agent using the Codex App Server, a bidirectional JSON-RPC API powering streaming progress, tool use, approvals, and diffs.",
      "published_at": "Wed, 04 Feb 2026 13:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "title": "VfL Wolfsburg turns ChatGPT into a club-wide capability",
      "url": "https://openai.com/index/vfl-wolfsburg",
      "summary": "By focusing on people, not pilots, the Bundesliga club is scaling efficiency, creativity, and knowledge—without losing its football identity.",
      "published_at": "Wed, 04 Feb 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-developers-blog",
      "source_name": "OpenAI Developers Blog",
      "title": "Docs MCP",
      "url": "https://developers.openai.com/resources/docs-mcp/",
      "summary": "Quickstart for connecting to OpenAI's developer docs MCP server from common editors. - mcp, tools, docs",
      "published_at": "Tue, 06 Jan 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-developers-blog",
      "source_name": "OpenAI Developers Blog",
      "title": "Gpt-image-1.5 Prompting Guide",
      "url": "https://developers.openai.com/cookbook/examples/multimodal/image-gen-1.5-prompting_guide",
      "summary": "Cookbook to prompt gpt-image-1.5 for reliable image generation results.",
      "published_at": "Tue, 16 Dec 2025 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-developers-blog",
      "source_name": "OpenAI Developers Blog",
      "title": "GPT-5.2 Prompting Guide",
      "url": "https://developers.openai.com/cookbook/examples/gpt-5/gpt-5-2_prompting_guide/",
      "summary": "Cookbook to prompt GPT-5.2 for accurate, concise enterprise workflows.",
      "published_at": "Thu, 11 Dec 2025 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-developers-blog",
      "source_name": "OpenAI Developers Blog",
      "title": "Codex Prompting Guide",
      "url": "https://developers.openai.com/cookbook/examples/gpt-5/codex_prompting_guide/",
      "summary": "Codex models advance the frontier of intelligence and efficiency and our recommended agentic coding model. Follow this guide closely to ensure you’re getting th",
      "published_at": "Thu, 04 Dec 2025 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-developers-blog",
      "source_name": "OpenAI Developers Blog",
      "title": "Transcribing User Audio with a Separate Realtime Request",
      "url": "https://developers.openai.com/cookbook/examples/realtime_out_of_band_transcription/",
      "summary": "Cookbook to transcribe user audio using out-of-band Realtime sessions.",
      "published_at": "Thu, 20 Nov 2025 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "openai-developers-blog",
      "source_name": "OpenAI Developers Blog",
      "title": "Modernizing your Codebase with Codex",
      "url": "https://developers.openai.com/cookbook/examples/codex/code_modernization/",
      "summary": "Cookbook to modernize legacy codebases using the OpenAI Codex CLI.",
      "published_at": "Wed, 19 Nov 2025 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Building a C compiler with a team of parallel Claudes Feb 05, 2026",
      "url": "https://www.anthropic.com/engineering/building-c-compiler",
      "summary": "",
      "published_at": "2026-02-05T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Designing AI-resistant technical evaluations Jan 21, 2026",
      "url": "https://www.anthropic.com/engineering/AI-resistant-technical-evaluations",
      "summary": "",
      "published_at": "2026-01-21T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Demystifying evals for AI agents Jan 09, 2026",
      "url": "https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents",
      "summary": "",
      "published_at": "2026-01-09T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Effective harnesses for long-running agents Nov 26, 2025",
      "url": "https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents",
      "summary": "",
      "published_at": "2025-11-26T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Introducing advanced tool use on the Claude Developer Platform Nov 24, 2025",
      "url": "https://www.anthropic.com/engineering/advanced-tool-use",
      "summary": "",
      "published_at": "2025-11-24T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Code execution with MCP: Building more efficient agents Nov 04, 2025",
      "url": "https://www.anthropic.com/engineering/code-execution-with-mcp",
      "summary": "",
      "published_at": "2025-11-04T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Beyond permission prompts: making Claude Code more secure and autonomous Oct 20, 2025",
      "url": "https://www.anthropic.com/engineering/claude-code-sandboxing",
      "summary": "",
      "published_at": "2025-10-20T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Effective context engineering for AI agents Sep 29, 2025",
      "url": "https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents",
      "summary": "",
      "published_at": "2025-09-29T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "A postmortem of three recent issues Sep 17, 2025",
      "url": "https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues",
      "summary": "",
      "published_at": "2025-09-17T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Writing effective tools for agents — with agents Sep 11, 2025",
      "url": "https://www.anthropic.com/engineering/writing-tools-for-agents",
      "summary": "",
      "published_at": "2025-09-11T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Desktop Extensions: One-click MCP server installation for Claude Desktop Jun 26, 2025",
      "url": "https://www.anthropic.com/engineering/desktop-extensions",
      "summary": "",
      "published_at": "2025-06-26T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "How we built our multi-agent research system Jun 13, 2025",
      "url": "https://www.anthropic.com/engineering/multi-agent-research-system",
      "summary": "",
      "published_at": "2025-06-13T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Claude Code: Best practices for agentic coding Apr 18, 2025",
      "url": "https://www.anthropic.com/engineering/claude-code-best-practices",
      "summary": "",
      "published_at": "2025-04-18T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "The \"think\" tool: Enabling Claude to stop and think in complex tool use situations Mar 20, 2025",
      "url": "https://www.anthropic.com/engineering/claude-think-tool",
      "summary": "",
      "published_at": "2025-03-20T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Raising the bar on SWE-bench Verified with Claude 3.5 Sonnet Jan 06, 2025",
      "url": "https://www.anthropic.com/engineering/swe-bench-sonnet",
      "summary": "",
      "published_at": "2025-01-06T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Building effective agents Dec 19, 2024",
      "url": "https://www.anthropic.com/engineering/building-effective-agents",
      "summary": "",
      "published_at": "2024-12-19T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Introducing Contextual Retrieval Sep 19, 2024",
      "url": "https://www.anthropic.com/engineering/contextual-retrieval",
      "summary": "",
      "published_at": "2024-09-19T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "title": "Featured Quantifying infrastructure noise in agentic coding evals Infrastructure configuration can swing agentic coding benchmarks by several percentage points—sometimes more than the leaderboard gap between top models.",
      "url": "https://www.anthropic.com/engineering/infrastructure-noise",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Bringing automated preview, review, and merge to Claude Code on desktop",
      "url": "https://claude.com/blog/preview-review-and-merge-with-claude-code",
      "summary": "",
      "published_at": "2026-02-20T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Increase web search accuracy and efficiency with dynamic filtering",
      "url": "https://claude.com/blog/improved-web-search-with-dynamic-filtering",
      "summary": "",
      "published_at": "2026-02-17T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Claude Enterprise, now available self-serve",
      "url": "https://claude.com/blog/self-serve-enterprise",
      "summary": "",
      "published_at": "2026-02-12T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Advancing finance with Claude Opus 4.6",
      "url": "https://claude.com/blog/opus-4-6-finance",
      "summary": "",
      "published_at": "2026-02-05T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Customize Cowork with plugins",
      "url": "https://claude.com/blog/cowork-plugins",
      "summary": "",
      "published_at": "2026-01-30T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "A complete guide to building skills for Claude",
      "url": "https://claude.com/blog/complete-guide-to-building-skills-for-claude",
      "summary": "",
      "published_at": "2026-01-29T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Understand Claude Code’s impact with contribution metrics",
      "url": "https://claude.com/blog/contribution-metrics",
      "summary": "",
      "published_at": "2026-01-29T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Updates to Claude Team",
      "url": "https://claude.com/blog/claude-team-updates",
      "summary": "",
      "published_at": "2026-01-28T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Your favorite work tools are now interactive connectors inside Claude",
      "url": "https://claude.com/blog/interactive-tools-in-claude",
      "summary": "",
      "published_at": "2026-01-26T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Building multi-agent systems: When and how to use them",
      "url": "https://claude.com/blog/building-multi-agent-systems-when-and-how-to-use-them",
      "summary": "",
      "published_at": "2026-01-23T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Building agents with Skills: Equipping agents for specialized work",
      "url": "https://claude.com/blog/building-agents-with-skills-equipping-agents-for-specialized-work",
      "summary": "",
      "published_at": "2026-01-22T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Eight trends defining how software gets built in 2026",
      "url": "https://claude.com/blog/eight-trends-defining-how-software-gets-built-in-2026",
      "summary": "",
      "published_at": "2026-01-21T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Cowork: Claude Code for the rest of your work",
      "url": "https://claude.com/blog/cowork-research-preview",
      "summary": "",
      "published_at": "2026-01-12T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "How enterprises are building AI agents in 2026",
      "url": "https://claude.com/blog/how-enterprises-are-building-ai-agents-in-2026",
      "summary": "",
      "published_at": "2025-12-09T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Improving frontend design through Skills",
      "url": "https://claude.com/blog/improving-frontend-design-through-skills",
      "summary": "",
      "published_at": "2025-11-12T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Building AI agents for financial services",
      "url": "https://claude.com/blog/building-ai-agents-in-financial-services",
      "summary": "",
      "published_at": "2025-10-30T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Claude Code on the web",
      "url": "https://claude.com/blog/claude-code-on-the-web",
      "summary": "",
      "published_at": "2025-10-20T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Claude and Slack",
      "url": "https://claude.com/blog/claude-and-slack",
      "summary": "",
      "published_at": "2025-10-01T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "Piloting Claude in Chrome",
      "url": "https://claude.com/blog/claude-for-chrome",
      "summary": "",
      "published_at": "2025-08-25T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "title": "How Anthropic teams use Claude Code",
      "url": "https://claude.com/blog/how-anthropic-teams-use-claude-code",
      "summary": "",
      "published_at": "2025-07-24T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Manus is now part of Meta",
      "url": "https://manus.im/blog/manus-joins-meta-for-next-era-of-innovation",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Vibecoding with Manus: From Beginner to Advanced Project Ideas Resources · February 17, 2026",
      "url": "https://manus.im/blog/vibe-code-projects",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Introducing Manus in Your Chat : Your Personal Agent, Everywhere You Are Product · February 16, 2026",
      "url": "https://manus.im/blog/manus-agents-telegram",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Introducing Project Skills: Turn Your Team’s Expertise into a Reusable Asset Product · February 12, 2026",
      "url": "https://manus.im/blog/manus-project-skills",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Beginner Prompts to Get Started with Manus Resources · February 11, 2026",
      "url": "https://manus.im/blog/manus-beginner-prompts",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Manus AI Embraces Open Standards: Integrating Agent Skills to Usher in a New Chapter for Agents Product · January 27, 2026",
      "url": "https://manus.im/blog/manus-skills",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Introducing App Sharing and Testing: Your Path to Google Play Store and App Store Product · January 19, 2026",
      "url": "https://manus.im/blog/manus-app-publishing",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Understanding Manus sandbox - your cloud computer Product · January 14, 2026",
      "url": "https://manus.im/blog/manus-sandbox",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Manus Partners with Similarweb for Digital Marketing Expertise Product · January 13, 2026",
      "url": "https://manus.im/blog/similarweb-manus",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Introducing Meeting Minutes Product · January 12, 2026",
      "url": "https://manus.im/blog/manus-meeting-minutes",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Introducing Slack Connector on Manus Product · December 31, 2025",
      "url": "https://manus.im/blog/manus-slack-connector",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Introducing Manus Design View Product · December 22, 2025",
      "url": "https://manus.im/blog/manus-design-view",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Manus Academy Launch Academy · December 19, 2025",
      "url": "https://manus.im/blog/manus-academy-launch",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Manus Projects Just Got Smarter with Connectors Product · December 19, 2025",
      "url": "https://manus.im/blog/projects-connectors",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Slides Created with Nano Banana Pro Are Now Editable—A First for AI Presentations Product · December 18, 2025",
      "url": "https://manus.im/blog/edit-slides-created-on-manus-with-nano-banana-pro",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Manus Update: $100M ARR, $125M revenue run-rate Product · December 17, 2025",
      "url": "https://manus.im/blog/manus-100m-arr",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Introducing Manus 1.6: Max Performance, Mobile Dev, and Design View Product · December 15, 2025",
      "url": "https://manus.im/blog/manus-max-release",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Get Your Manus App to Rank on Search Engines: Introducing Built-in SEO Product · December 10, 2025",
      "url": "https://manus.im/blog/manus-advanced-seo",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Google Drive Connector on Manus Product · December 8, 2025",
      "url": "https://manus.im/blog/manus-google-drive-connector",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "title": "Introducing Custom Domains: Your App, Your Address Product · December 8, 2025",
      "url": "https://manus.im/blog/manus-custom-domains",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "How to Use Memory in Agent Builder",
      "url": "https://blog.langchain.com/how-to-use-memory-in-agent-builder/",
      "summary": "<p><em>By Jacob Talbot</em></p><p>Agent Builder gets better the more you use it because it remembers your feedback. Every correction you make, preference you share, and approach that works well is something that your agent can hold onto and apply the next time.</p><p>Memory is one of the things that makes</p>",
      "published_at": "Thu, 19 Feb 2026 18:28:14 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "New in Agent Builder: all new agent chat, file uploads + tool registry",
      "url": "https://blog.langchain.com/new-in-agent-builder-all-new-agent-chat-file-uploads-tool-registry/",
      "summary": "<p>Today, we&apos;re expanding what you can do with <a href=\"https://www.langchain.com/langsmith/agent-builder?ref=blog.langchain.com\">LangSmith Agent Builder</a>. It&#x2019;s an big update built around a simple idea: working with an agent should feel like working with a teammate.</p><p>We rebuilt Agent Builder around this idea. There is now an always available agent (&#x201d;</p>",
      "published_at": "Wed, 18 Feb 2026 15:55:08 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "monday Service + LangSmith: Building a Code-First Evaluation Strategy from Day 1",
      "url": "https://blog.langchain.com/customers-monday/",
      "summary": "Learn how monday Service developed an eval-driven development framework for their customer-facing service agents.",
      "published_at": "Wed, 18 Feb 2026 08:05:53 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "Improving Deep Agents with harness engineering",
      "url": "https://blog.langchain.com/improving-deep-agents-with-harness-engineering/",
      "summary": "<p>TLDR: Our coding agent went from Top 30 to Top 5 on <a href=\"https://www.tbench.ai/leaderboard/terminal-bench/2.0?ref=blog.langchain.com\">Terminal Bench 2.0</a>. We only changed the harness. Here&#x2019;s our approach to harness engineering (teaser: self-verification &amp; tracing help a lot).</p><h2 id=\"the-goal-of-harness-engineering\">The Goal of Harness Engineering</h2><p>The goal of a harness is to mold the</p>",
      "published_at": "Tue, 17 Feb 2026 16:15:28 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "On Agent Frameworks and Agent Observability",
      "url": "https://blog.langchain.com/on-agent-frameworks-and-agent-observability/",
      "summary": "<p>Every time LLMs get better, the same question comes back: &quot;Do you still need an agent framework?&quot; It&apos;s a fair question. The best way to build agents changes as the models get more performant and evolve, but fundamentally, the agent is a system <em>around</em> the model,</p>",
      "published_at": "Fri, 13 Feb 2026 02:23:40 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "Join us for Interrupt: The Agent Conference",
      "url": "https://blog.langchain.com/join-us-for-interrupt-the-agent-conference/",
      "summary": "<p>Interrupt - The Agent Conference by LangChain - is where builders come to learn what&apos;s actually working in production. This year, we&apos;re bringing together more than 1,000 developers, product leaders, researchers, and founders to share what&apos;s coming next for agents&#x2014;and how</p>",
      "published_at": "Thu, 12 Feb 2026 17:42:03 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "The two patterns by which agents connect sandboxes",
      "url": "https://blog.langchain.com/the-two-patterns-by-which-agents-connect-sandboxes/",
      "summary": "<p><em>Thank you to Nuno Campos from Witan Labs, Tomas Beran and Mikayel Harutyunyan from E2B, Jonathan Wall from Runloop, and Ben Guo from Zo Computer for their review and comments.</em></p><p><strong>TL;DR:</strong></p><ul><li><strong>More and more agents need a workspace: a computer where they can run code, install packages, and access</strong></li></ul>",
      "published_at": "Tue, 10 Feb 2026 16:32:35 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "LangSmith is Now Available in Google Cloud Marketplace",
      "url": "https://blog.langchain.com/langsmith-is-now-available-in-google-cloud-marketplace/",
      "summary": "<p>Today, we&apos;re thrilled to announce that LangSmith, the agent engineering platform from LangChain, is available in Google Cloud Marketplace. Google Cloud customers can now procure LangSmith through their existing Google Cloud accounts, enabling seamless billing, simplified procurement, and the ability to draw down on existing Google Cloud commitments.</p>",
      "published_at": "Tue, 10 Feb 2026 02:47:44 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "January 2026: LangChain Newsletter",
      "url": "https://blog.langchain.com/january-2026-langchain-newsletter/",
      "summary": "Read about the latest product updates, events, and content from the LangChain team",
      "published_at": "Fri, 30 Jan 2026 02:27:28 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "Context Management for Deep Agents",
      "url": "https://blog.langchain.com/context-management-for-deepagents/",
      "summary": "<p><em>By Chester Curme and Mason Daugherty</em></p><p>As the addressable task length of AI agents <a href=\"https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/?ref=blog.langchain.com\">continues to grow</a>, effective context management becomes critical to prevent <a href=\"https://research.trychroma.com/context-rot?ref=blog.langchain.com\">context rot</a> and to manage LLMs&#x2019; finite memory constraints.</p><p>The <a href=\"https://docs.langchain.com/oss/python/deepagents/overview?ref=blog.langchain.com\">Deep Agents SDK</a> is LangChain&#x2019;s open source, batteries-included <a href=\"https://blog.langchain.com/agent-frameworks-runtimes-and-harnesses-oh-my/\">agent harness</a>. It provides an</p>",
      "published_at": "Wed, 28 Jan 2026 16:11:29 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "Deploy agents instantly with Agent Builder templates",
      "url": "https://blog.langchain.com/introducing-agent-builder-template-library/",
      "summary": "Introducing the Agent Builder Template Library: a collection of ready-to-deploy agents for common tasks, equipped with the tools you already use.",
      "published_at": "Wed, 21 Jan 2026 17:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "Building Multi-Agent Applications with Deep Agents",
      "url": "https://blog.langchain.com/building-multi-agent-applications-with-deep-agents/",
      "summary": "Breaking down complex tasks across specialized agents is one of the most effective approaches to building capable AI systems. In this post, we'll show you how to build multi-agent systems with Deep Agents.",
      "published_at": "Wed, 21 Jan 2026 16:30:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "From Traces to Insights: Understanding Agent Behavior at Scale",
      "url": "https://blog.langchain.com/from-traces-to-insights-understanding-agent-behavior-at-scale/",
      "summary": "<blockquote>Visibility is the easiest piece. The hard part is analyzing and understanding what you&#x2019;re observing. I&#x2019;ve spoken to teams recording 100k+ traces every single day. What are they doing with those traces? Literally nothing. Because it&#x2019;s impossible to read and summarize 100,000 traces</blockquote>",
      "published_at": "Tue, 20 Jan 2026 17:16:56 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "How Remote uses LangChain and LangGraph to onboard thousands of customers with AI",
      "url": "https://blog.langchain.com/customers-remote/",
      "summary": "<p><em>Guest post written by Jos&#xe9; Mussa (Staff Software Engineer @ Remote)</em></p><p><a href=\"https://remote.com/?ref=blog.langchain.com\"><u>Remote</u></a> is a fast-growing startup helping companies hire, manage, and pay employees globally from a single platform. Remote&#x2019;s customers operate across many countries and regulatory environments, and they trust Remote as the system of record for their</p>",
      "published_at": "Mon, 19 Jan 2026 16:00:07 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "title": "Choosing the Right Multi-Agent Architecture",
      "url": "https://blog.langchain.com/choosing-the-right-multi-agent-architecture/",
      "summary": "In this post, we’ll explore when multi-agent architectures become necessary, the four main patterns we’ve observed, and how LangChain empowers you to effectively build multi-agent systems.",
      "published_at": "Wed, 14 Jan 2026 18:06:14 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "GGML and llama.cpp join HF to ensure the long-term progress of Local AI",
      "url": "https://huggingface.co/blog/ggml-joins-hf",
      "summary": "",
      "published_at": "Fri, 20 Feb 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "Train AI models with Unsloth and Hugging Face Jobs for FREE",
      "url": "https://huggingface.co/blog/unsloth-jobs",
      "summary": "",
      "published_at": "Fri, 20 Feb 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "「データ不足」の壁を越える：合成ペルソナが日本のAI開発を加速",
      "url": "https://huggingface.co/blog/nvidia/nemotron-personas-japan-nttdata-ja",
      "summary": "",
      "published_at": "Thu, 19 Feb 2026 15:32:38 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "IBM and UC Berkeley Diagnose Why Enterprise Agents Fail Using IT-Bench and MAST",
      "url": "https://huggingface.co/blog/ibm-research/itbenchandmast",
      "summary": "",
      "published_at": "Wed, 18 Feb 2026 16:15:45 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "One-Shot Any Web App with Gradio's gr.HTML",
      "url": "https://huggingface.co/blog/gradio-html-one-shot-apps",
      "summary": "",
      "published_at": "Wed, 18 Feb 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "Custom Kernels for All from Codex and Claude",
      "url": "https://huggingface.co/blog/custom-cuda-kernels-agent-skills",
      "summary": "",
      "published_at": "Fri, 13 Feb 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "OpenEnv in Practice: Evaluating Tool-Using Agents in Real-World Environments",
      "url": "https://huggingface.co/blog/openenv-turing",
      "summary": "",
      "published_at": "Thu, 12 Feb 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "Transformers.js v4 Preview: Now Available on NPM!",
      "url": "https://huggingface.co/blog/transformersjs-v4",
      "summary": "",
      "published_at": "Mon, 09 Feb 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "Introducing SyGra Studio",
      "url": "https://huggingface.co/blog/ServiceNow-AI/sygra-studio",
      "summary": "",
      "published_at": "Thu, 05 Feb 2026 16:52:28 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "Community Evals: Because we're done trusting black-box leaderboards over the community",
      "url": "https://huggingface.co/blog/community-evals",
      "summary": "",
      "published_at": "Wed, 04 Feb 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "H Company's new Holo2 model takes the lead in UI Localization",
      "url": "https://huggingface.co/blog/Hcompany/introducing-holo2-235b-a22b",
      "summary": "",
      "published_at": "Tue, 03 Feb 2026 17:40:14 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "The Future of the Global Open-Source AI Ecosystem: From DeepSeek to AI+",
      "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-3",
      "summary": "",
      "published_at": "Tue, 03 Feb 2026 15:03:19 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "Training Design for Text-to-Image Models: Lessons from Ablations",
      "url": "https://huggingface.co/blog/Photoroom/prx-part2",
      "summary": "",
      "published_at": "Tue, 03 Feb 2026 11:25:53 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "Introducing Daggr: Chain apps programmatically, inspect visually",
      "url": "https://huggingface.co/blog/daggr",
      "summary": "",
      "published_at": "Thu, 29 Jan 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "We Got Claude to Build CUDA Kernels and teach open models!",
      "url": "https://huggingface.co/blog/upskill",
      "summary": "",
      "published_at": "Wed, 28 Jan 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "Architectural Choices in China's Open-Source AI Ecosystem: Building Beyond DeepSeek",
      "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-2",
      "summary": "",
      "published_at": "Tue, 27 Jan 2026 15:01:45 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "Alyah ⭐️: Toward Robust Evaluation of Emirati Dialect Capabilities in Arabic LLMs",
      "url": "https://huggingface.co/blog/tiiuae/emirati-benchmarks",
      "summary": "",
      "published_at": "Tue, 27 Jan 2026 10:26:42 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "Unlocking Agentic RL Training for GPT-OSS: A Practical Retrospective",
      "url": "https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl",
      "summary": "",
      "published_at": "Tue, 27 Jan 2026 01:53:15 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "AssetOpsBench: Bridging the Gap Between AI Agent Benchmarks and Industrial Reality",
      "url": "https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face",
      "summary": "",
      "published_at": "Wed, 21 Jan 2026 06:25:31 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "title": "One Year Since the “DeepSeek Moment”",
      "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment",
      "summary": "",
      "published_at": "Tue, 20 Jan 2026 15:02:10 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "github-blog",
      "source_name": "GitHub Blog",
      "title": "How AI is reshaping developer choice (and Octoverse data proves it)",
      "url": "https://github.blog/ai-and-ml/generative-ai/how-ai-is-reshaping-developer-choice-and-octoverse-data-proves-it/",
      "summary": "<p>AI is rewiring developer preferences through convenience loops. Octoverse 2025 reveals how AI compatibility is becoming the new standard for technology choice.</p> <p>The post <a href=\"https://github.blog/ai-and-ml/generative-ai/how-ai-is-reshaping-developer-choice-and-octoverse-data-proves-it/\">How AI is reshaping developer choice (and Octoverse data proves it)</a> appeared first on <a href=\"https://github.blog\">The GitHub Blog</a>.</p>",
      "published_at": "Thu, 19 Feb 2026 17:00:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "github-blog",
      "source_name": "GitHub Blog",
      "title": "What to expect for open source in 2026",
      "url": "https://github.blog/open-source/maintainers/what-to-expect-for-open-source-in-2026/",
      "summary": "<p>Let’s dig into the 2025’s open source data on GitHub to see what we can learn about the future.</p> <p>The post <a href=\"https://github.blog/open-source/maintainers/what-to-expect-for-open-source-in-2026/\">What to expect for open source in 2026</a> appeared first on <a href=\"https://github.blog\">The GitHub Blog</a>.</p>",
      "published_at": "Wed, 18 Feb 2026 18:41:42 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "github-blog",
      "source_name": "GitHub Blog",
      "title": "Securing the AI software supply chain: Security results across 67 open source projects",
      "url": "https://github.blog/open-source/maintainers/securing-the-ai-software-supply-chain-security-results-across-67-open-source-projects/",
      "summary": "<p>Learn how The GitHub Secure Open Source Fund helped 67 critical AI‑stack projects accelerate fixes, strengthen ecosystems, and advance open source resilience.</p> <p>The post <a href=\"https://github.blog/open-source/maintainers/securing-the-ai-software-supply-chain-security-results-across-67-open-source-projects/\">Securing the AI software supply chain: Security results across 67 open source projects</a> appeared first on <a href=\"https://github.blog\">The GitHub Blog</a>.</p>",
      "published_at": "Tue, 17 Feb 2026 19:00:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "github-blog",
      "source_name": "GitHub Blog",
      "title": "Automate repository tasks with GitHub Agentic Workflows",
      "url": "https://github.blog/ai-and-ml/automate-repository-tasks-with-github-agentic-workflows/",
      "summary": "<p>Discover GitHub Agentic Workflows, now in technical preview. Build automations using coding agents in GitHub Actions to handle triage, documentation, code quality, and more.</p> <p>The post <a href=\"https://github.blog/ai-and-ml/automate-repository-tasks-with-github-agentic-workflows/\">Automate repository tasks with GitHub Agentic Workflows </a> appeared first on <a href=\"https://github.blog\">The GitHub Blog</a>.</p>",
      "published_at": "Fri, 13 Feb 2026 14:00:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "github-blog",
      "source_name": "GitHub Blog",
      "title": "Welcome to the Eternal September of open source. Here’s what we plan to do for maintainers.",
      "url": "https://github.blog/open-source/maintainers/welcome-to-the-eternal-september-of-open-source-heres-what-we-plan-to-do-for-maintainers/",
      "summary": "<p>Open source is hitting an “Eternal September.” As contribution friction drops, maintainers are adapting with new trust signals, triage approaches, and community-led solutions.</p> <p>The post <a href=\"https://github.blog/open-source/maintainers/welcome-to-the-eternal-september-of-open-source-heres-what-we-plan-to-do-for-maintainers/\">Welcome to the Eternal September of open source. Here&#8217;s what we plan to do for maintainers.</a> appeared first on <a href=\"https://github.blog\">The GitHub Blog</a>.</p>",
      "published_at": "Thu, 12 Feb 2026 20:14:11 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "github-blog",
      "source_name": "GitHub Blog",
      "title": "GitHub availability report: January 2026",
      "url": "https://github.blog/news-insights/company-news/github-availability-report-january-2026/",
      "summary": "<p>In January, we experienced two incidents that resulted in degraded performance across GitHub services.</p> <p>The post <a href=\"https://github.blog/news-insights/company-news/github-availability-report-january-2026/\">GitHub availability report: January 2026</a> appeared first on <a href=\"https://github.blog\">The GitHub Blog</a>.</p>",
      "published_at": "Wed, 11 Feb 2026 23:12:34 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "github-blog",
      "source_name": "GitHub Blog",
      "title": "Continuous AI in practice: What developers can automate today with agentic CI",
      "url": "https://github.blog/ai-and-ml/generative-ai/continuous-ai-in-practice-what-developers-can-automate-today-with-agentic-ci/",
      "summary": "<p>Think of Continuous AI as background agents that operate in your repository for tasks that require reasoning.</p> <p>The post <a href=\"https://github.blog/ai-and-ml/generative-ai/continuous-ai-in-practice-what-developers-can-automate-today-with-agentic-ci/\">Continuous AI in practice: What developers can automate today with agentic CI</a> appeared first on <a href=\"https://github.blog\">The GitHub Blog</a>.</p>",
      "published_at": "Thu, 05 Feb 2026 17:00:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "github-blog",
      "source_name": "GitHub Blog",
      "title": "Pick your agent: Use Claude and Codex on Agent HQ",
      "url": "https://github.blog/news-insights/company-news/pick-your-agent-use-claude-and-codex-on-agent-hq/",
      "summary": "<p>Claude by Anthropic and OpenAI Codex are now available in public preview on GitHub and VS Code with a Copilot Pro+ or Copilot Enterprise subscription. Here's what you need to know and how to get started today.</p> <p>The post <a href=\"https://github.blog/news-insights/company-news/pick-your-agent-use-claude-and-codex-on-agent-hq/\">Pick your agent: Use Claude and Codex on Agent HQ </a> appeared first on <a href=\"https://github.blog\">The GitHub Blog</a>.</p>",
      "published_at": "Wed, 04 Feb 2026 17:00:19 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "github-blog",
      "source_name": "GitHub Blog",
      "title": "What the fastest-growing tools reveal about how software is being built",
      "url": "https://github.blog/news-insights/octoverse/what-the-fastest-growing-tools-reveal-about-how-software-is-being-built/",
      "summary": "<p>What languages are growing fastest, and why? What about the projects that people are interested in the most? Where are new developers cutting their teeth? Let’s take a look at Octoverse data to find out.</p> <p>The post <a href=\"https://github.blog/news-insights/octoverse/what-the-fastest-growing-tools-reveal-about-how-software-is-being-built/\">What the fastest-growing tools reveal about how software is being built</a> appeared first on <a href=\"https://github.blog\">The GitHub Blog</a>.</p>",
      "published_at": "Tue, 03 Feb 2026 17:00:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "github-blog",
      "source_name": "GitHub Blog",
      "title": "How to maximize GitHub Copilot’s agentic capabilities",
      "url": "https://github.blog/ai-and-ml/github-copilot/how-to-maximize-github-copilots-agentic-capabilities/",
      "summary": "<p>A senior engineer's guide to architecting and extending Copilot's real-world applications.</p> <p>The post <a href=\"https://github.blog/ai-and-ml/github-copilot/how-to-maximize-github-copilots-agentic-capabilities/\">How to maximize GitHub Copilot&#8217;s agentic capabilities</a> appeared first on <a href=\"https://github.blog\">The GitHub Blog</a>.</p>",
      "published_at": "Mon, 02 Feb 2026 17:00:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "Cloudflare outage on February 20, 2026",
      "url": "https://blog.cloudflare.com/cloudflare-outage-february-20-2026/",
      "summary": "Cloudflare suffered a service outage on February 20, 2026. A subset of customers who use Cloudflare’s Bring Your Own IP (BYOIP) service saw their routes to the Internet withdrawn via Border Gateway Protocol (BGP).",
      "published_at": "Sat, 21 Feb 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "Code Mode: give agents an entire API in 1,000 tokens",
      "url": "https://blog.cloudflare.com/code-mode-mcp/",
      "summary": "The Cloudflare API has over 2,500 endpoints. Exposing each one as an MCP tool would consume over 2 million tokens. With Code Mode, we collapsed all of it into two tools and roughly 1,000 tokens of context.",
      "published_at": "Fri, 20 Feb 2026 14:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "Shedding old code with ecdysis: graceful restarts for Rust services at Cloudflare",
      "url": "https://blog.cloudflare.com/ecdysis-rust-graceful-restarts/",
      "summary": "ecdysis is a Rust library enabling zero-downtime upgrades for network services. After five years protecting millions of connections at Cloudflare, it’s now open source.",
      "published_at": "Fri, 13 Feb 2026 14:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "Introducing Markdown for Agents",
      "url": "https://blog.cloudflare.com/markdown-for-agents/",
      "summary": "The way content is discovered online is shifting, from traditional search engines to AI agents that need structured data from a Web built for humans. It’s time to consider not just human visitors, but start to treat agents as first-class citizens. Markdown for Agents automatically converts any HTML page requested from our network to markdown.",
      "published_at": "Thu, 12 Feb 2026 14:03:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "2025 Q4 DDoS threat report: A record-setting 31.4 Tbps attack caps a year of massive DDoS assaults",
      "url": "https://blog.cloudflare.com/ddos-threat-report-2025-q4/",
      "summary": "The number of DDoS attacks more than doubled in 2025. The network layer is under particular threat as hyper-volumetric attacks grew 700%.",
      "published_at": "Thu, 05 Feb 2026 14:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "Improve global upload performance with R2 Local Uploads",
      "url": "https://blog.cloudflare.com/r2-local-uploads/",
      "summary": "Local Uploads on R2 reduces request duration for uploads by up to 75%. It writes object data to a nearby location and asynchronously copies it to your bucket, all while data is available immediately.",
      "published_at": "Tue, 03 Feb 2026 14:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "Google’s AI advantage: why crawler separation is the only path to a fair Internet",
      "url": "https://blog.cloudflare.com/uk-google-ai-crawler-policy/",
      "summary": "Google's dual-purpose crawler creates an unfair AI advantage. To protect publishers and foster competition, the UK’s Competition and Markets Authority must mandate crawler separation for search and AI.",
      "published_at": "Fri, 30 Jan 2026 17:01:04 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "Building vertical microfrontends on Cloudflare’s platform",
      "url": "https://blog.cloudflare.com/vertical-microfrontends/",
      "summary": "Deploy multiple Workers under a single domain with the ability to make them feel like single-page applications. We take a look at how service bindings enable URL path routing to multiple projects.",
      "published_at": "Fri, 30 Jan 2026 14:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "Introducing Moltworker: a self-hosted personal AI agent, minus the minis",
      "url": "https://blog.cloudflare.com/moltworker-self-hosted-ai-agent/",
      "summary": "Moltworker is a middleware Worker and adapted scripts that allows running OpenClaw (formerly Moltbot, formerly Clawdbot) on Cloudflare's Sandbox SDK and our Developer Platform APIs. So you can self-host an AI personal assistant — without any new hardware.",
      "published_at": "Thu, 29 Jan 2026 14:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "Building a serverless, post-quantum Matrix homeserver",
      "url": "https://blog.cloudflare.com/serverless-matrix-homeserver-workers/",
      "summary": "As a proof of concept, we built a Matrix homeserver to Cloudflare Workers — delivering encrypted messaging at the edge with automatic post-quantum cryptography.",
      "published_at": "Tue, 27 Jan 2026 14:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "Cable cuts, storms, and DNS: a look at Internet disruptions in Q4 2025",
      "url": "https://blog.cloudflare.com/q4-2025-internet-disruption-summary/",
      "summary": "The last quarter of 2025 brought several notable disruptions to Internet connectivity. Cloudflare Radar data reveals the impact of cable cuts, power outages, extreme weather, technical problems, and more.",
      "published_at": "Mon, 26 Jan 2026 14:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "Route leak incident on January 22, 2026",
      "url": "https://blog.cloudflare.com/route-leak-incident-january-22-2026/",
      "summary": "An automated routing policy configuration error caused us to leak some Border Gateway Protocol prefixes unintentionally from a router at our Miami data center. We discuss the impact and the changes we are implementing as a result.",
      "published_at": "Fri, 23 Jan 2026 14:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "How we mitigated a vulnerability in Cloudflare’s ACME validation logic",
      "url": "https://blog.cloudflare.com/acme-path-vulnerability/",
      "summary": "A vulnerability was recently identified in Cloudflare’s automation of certificate validation. Here we explain the vulnerability and outline the steps we’ve taken to mitigate it.",
      "published_at": "Mon, 19 Jan 2026 14:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "Astro is joining Cloudflare",
      "url": "https://blog.cloudflare.com/astro-joins-cloudflare/",
      "summary": "The Astro Technology Company team — the creators of the Astro web framework — is joining Cloudflare. We’re doubling down on making Astro the best framework for content-driven websites, today and in the years to come.",
      "published_at": "Fri, 16 Jan 2026 14:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "Human Native is joining Cloudflare",
      "url": "https://blog.cloudflare.com/human-native-joins-cloudflare/",
      "summary": "Cloudflare acquires Human Native, an AI data marketplace specialising in transforming content into searchable and useful data, to accelerate work building new economic models for the Internet.",
      "published_at": "Thu, 15 Jan 2026 14:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "What came first: the CNAME or the A record?",
      "url": "https://blog.cloudflare.com/cname-a-record-order-dns-standards/",
      "summary": "A recent change to 1.1.1.1 accidentally altered the order of CNAME records in DNS responses, breaking resolution for some clients. This post explores the technical root cause, examines the source code of affected resolvers, and dives into the inherent ambiguities of the DNS RFCs.",
      "published_at": "Wed, 14 Jan 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "What we know about Iran’s Internet shutdown",
      "url": "https://blog.cloudflare.com/iran-protests-internet-shutdown/",
      "summary": "Cloudflare Radar data shows Internet traffic from Iran has effectively dropped to zero since January 8, signaling a complete shutdown in the country and disconnection from the global Internet.",
      "published_at": "Tue, 13 Jan 2026 00:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "A closer look at a BGP anomaly in Venezuela",
      "url": "https://blog.cloudflare.com/bgp-route-leak-venezuela/",
      "summary": "There has been speculation about the cause of a BGP anomaly observed in Venezuela on January 2. We take a look at BGP route leaks, and dive into what the data suggests caused the anomaly in question.",
      "published_at": "Tue, 06 Jan 2026 08:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "How Workers powers our internal maintenance scheduling pipeline",
      "url": "https://blog.cloudflare.com/building-our-maintenance-scheduler-on-workers/",
      "summary": "Physical data center maintenance is risky on a global network. We built a maintenance scheduler on Workers to safely plan disruptive operations, while solving scaling challenges by viewing the state of our infrastructure through a graph interface on top of multiple data sources and metrics pipelines.",
      "published_at": "Mon, 22 Dec 2025 14:00:00 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "title": "Code Orange: Fail Small — our resilience plan following recent incidents",
      "url": "https://blog.cloudflare.com/fail-small-resilience-plan/",
      "summary": "We have declared “Code Orange: Fail Small” to focus everyone at Cloudflare on a set of high-priority workstreams with one simple goal: ensure that the cause of our last two global outages never happens again.",
      "published_at": "Fri, 19 Dec 2025 22:35:30 GMT",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Skills Night: 69,000+ ways agents are getting smarter",
      "url": "https://vercel.com/blog/skills-night-69000-ways-agents-are-getting-smarter",
      "summary": "<p>The room was full of people who had already used skills.</p><p>Tuesday night we hosted Skills Night in San Francisco, an event for developers building on and around <a href=\"https://skills.sh\">skills.sh</a>, the open skills ecosystem we've been growing since the idea started as a single weekend of writing. What began as Shu Ding sitting down to document everything he knows about React has grown into over 69,000 skills, 2 million skill CLI installs, and a community moving incredibly fast.</p><div></div><div></div><p>Here is what we learned.</p><h3>Where this came from</h3><p>The origin story is worth retelling because it shapes how we think about the project.</p><p><a href=\"https://x.com/shuding\">Shu Ding</a> is one of the most talented web engineers I've ever worked with. He knows things about React and the browser that most people will never discover. Last year, he sat down on a weekend and wrote it all down. A kind of web bible. We wanted to figure out how to ship it. We considered a blog post or documentation that the next generation of models might eventually learn - but we wouldn't see the results until Claude Sonnet 8, or GPT-9. On the other hand, an MCP server felt too heavy for what was essentially a collection of markdown documents.</p><p>Skills made sense as the quickest way to deliver on-demand knowledge. While writing the instructions for installing React best practices, I ended up copying and pasting the same installation instructions for getting the skills into Cursor, Claude Code, Codex, and the other 10+ coding agents but with slightly different installation directories.</p><p>So I built a CLI to install it into every major coding agent at once. That became <code>npx skills</code>. We added telemetry to surface new skills as they got installed, which became the data that powers the leaderboard at <a href=\"https://skills.sh\">skills.sh</a>. The whole thing went from idea to production on Vercel in days. Malte Ubl, Vercel CTO, framed it perfectly: it's a package manager for agent context.</p><p>Now we are tracking 69,000 of them, and making them not just easy to discover but easy to install, with simple commands like just:</p><div></div><h3>The security problem we needed to solve</h3><p>Growth creates attack surface, and fast growth creates it even faster.</p><p>As soon as skills took off, quality variance followed. Ryan from Socket showed us a concrete example: a skill that looked completely clean at the markdown level but included a Python file that opened a remote shell on install. You would never catch that without looking at every file in the directory.</p><p>That is why we announced security partnerships with <a href=\"https://ai.gendigital.com/\">Gen</a>, <a href=\"https://socket.dev\">Socket</a>, and <a href=\"https://snyk.io\">Snyk</a> to run audits across all skills and every new one that comes in.</p><ul><li><p>Socket is doing cross-ecosystem static analysis combined with LLM-based noise reduction, reporting 95% precision, 98% recall, and 97% F1 across their benchmarks.</p></li><li><p>Gen is building a real-time agent trust layer called Sage that monitors every connection in and out of your agents, allowing them to run freely without risk of data exfiltration or prompt injection.</p></li><li><p>Snyk is bringing their package security background to the skills context.</p></li></ul><p>We are building an Audits leaderboard to provide per-skill assessments and recommendations. The goal is not to lock things down. The goal is to let you go fast with confidence. We're always looking for new security partners who can bring unique perspectives to auditing skills and provide more trust signals for skills.</p><h3>What the demos showed us</h3><p>Eight partners showed demos on Tuesday, and a few themes kept coming up.</p><p><b>Skills close the training cutoff gap.</b> <a href=\"https://x.com/davis7\">Ben Davis</a> ran a controlled experiment to demonstrate this.</p><p>He tried to get coding agents to implement Svelte remote functions, a relatively new API, four different ways: no context, a skills file with documentation, a skill pointing to the MCP, and a code example in the project. </p><p>Every approach with context worked.</p><p>The no-context run, which he had to force through a stripped-down model to prevent it from inferring solutions, produced completely wrong output. Models are smart enough to use patterns correctly when you give them the patterns. Without context, they fall back to stale training data.</p><p><b>The medium matters less than the content.</b> The interesting takeaway from Ben's experiment was not that skills are the only way. It is that getting the right context in is what matters, and skills are the fastest starting point if you do not already have a baseline. Existing code examples, inline documentation, and MCP hints all work. </p><p>Skills are just the easiest way to distribute that context to anyone.</p><p><b>Agents can now drive the whole stack.</b> <a href=\"https://x.com/Baconbrix\">Evan Bacon</a> from Expo showed native iOS feature upgrades driven entirely by Claude Code using Expo skills.</p><p>New SwiftUI components, gesture-driven transitions, and tab bar updates were all applied automatically. They are also using LLDB integration in a work-in-progress skill that lets agents read the native iOS view hierarchy and fix notoriously hard keyboard handling bugs automatically.</p><p>Their production app, Expo Go, now auto-fixes every crash as it occurs. For anyone who has spent time wrestling with Xcode, that is a significant statement.</p><p><b>Skills are becoming infrastructure.</b> <a href=\"https://x.com/skeptrune\">Nick Khami</a> showed off that Mintlify auto-generates a skill for every documentation site they host, including Claude Code's own docs, Coinbase, Perplexity, and Lovable.</p><p>Traffic to these sites is now 50% coding agents, up from 10% a year ago. The skill is not something the docs team writes anymore; it is a byproduct of having well-structured documentation. Sentry's David Cramer built Warden, a harness that runs skills as linters on pull requests via GitHub Actions, treating agents as a static analysis layer.</p><h3>What we're building toward</h3><p>Guillermo Rauch, Vercel CEO, said something Tuesday night that I keep thinking about: agents make mistakes.</p><p>They sometimes tell you you are absolutely right and proceed to do the wrong thing. Shipping quality in the AI era means not just celebrating how many tokens you are burning. It means raising the bar on what those tokens actually produce.</p><p>Skills are one answer to that problem. They are how we influence what agents create, keep them up to date with framework changes, and make them more token-efficient by giving them a straight path to the right answer instead of letting them stumble around.</p><p>Two million installs is real signal. The security partnerships make it something teams can rely on. And the demos showed that the most interesting skills work is not at the CLI level. It is in the agents and tools that are now treating skills as a first-class primitive for distributing knowledge at scale.</p><p>We will keep building. Come find us at <a href=\"https://skills.sh\">skills.sh</a>.</p><div></div><p></p> <p class=\"more\"> <a href=\"https://vercel.com/blog/skills-night-69000-ways-agents-are-getting-smarter\">Read more</a> </p>",
      "published_at": "2026-02-20T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Video Generation with AI Gateway",
      "url": "https://vercel.com/blog/video-generation-with-ai-gateway",
      "summary": "<p>AI Gateway now supports video generation, so you can create cinematic videos with photorealistic quality, synchronized audio, generate personalized content with consistent identity, all through AI SDK 6.</p><h2>Two ways to get started</h2><p>Video generation is in beta and currently available for Pro and Enterprise plans and paid AI Gateway users.</p><ul><li><p><b>AI SDK 6</b>: Generate videos programmatically with the same interface you use for text and images. One API, one authentication flow, one observability dashboard across your entire AI pipeline.</p></li></ul><div></div><ul><li><p><b>AI Gateway Playground</b>: Experiment with video models with no code in the configurable <a href=\"https://vercel.com/ai-gateway/models/grok-imagine-video\">AI Gateway playground</a> that's embedded in each model page. Compare providers, tweak prompts, and download results without writing code. To access, click any video gen model in the <a href=\"https://vercel.com/ai-gateway/models?capabilities=video-generation\">model list</a>.</p></li></ul><h2>Four initial video models; 17 variations</h2><ul><li><p><b>Grok Imagine</b> from xAI is fast and great at instruction following. Create and edit videos with style transfer, all in seconds.</p></li><li><p><b>Wan</b> from Alibaba specializes in reference-based generation and multi-shot storytelling, with the ability to preserve identity across scenes.</p></li><li><p><b>Kling</b> excels at image to video and native audio. The new 3.0 models support multishot video with automatic scene transitions.</p></li><li><p><b>Veo</b> from Google delivers high visual fidelity and physics realism. Native audio generation with cinematic lighting and physics.</p></li></ul><h2>Understanding video requests</h2><p>Video models require more than just describing what you want. Unlike image generation, video prompts can include motion cues (camera movement, object actions, timing) and optionally audio direction. Each provider exposes different capabilities through <code>providerOptions</code> that unlock fundamentally different generation modes. See the <a href=\"https://vercel.com/docs/ai-gateway/capabilities/video-generation\">documentation</a> for model-specific options.</p><h2>Generation types</h2><p>AI Gateway initially supports 4 types of video generation:</p><table><tr><td><p><b>Type</b></p></td><td><p><b>Inputs</b></p></td><td><p><b>Description</b></p></td><td><p><b>Example use cases</b></p></td></tr><tr><td><p>Text-to-video</p></td><td><p>Text prompt</p></td><td><p>Describe a scene, get a video</p></td><td><p>Ad creative, explainer videos, social content</p></td></tr><tr><td><p>Image-to-video</p></td><td><p>Image, text prompt optional</p></td><td><p>Animate a still image with motion</p></td><td><p>Product showcases, logo reveals, photo animation</p></td></tr><tr><td><p>First and last frame</p></td><td><p>2 images, text prompt optional</p></td><td><p>Define start and end states, model fills in between</p></td><td><p>Before/after reveals, time-lapse, transitions</p></td></tr><tr><td><p>Reference-to-video</p></td><td><p>Images or videos</p></td><td><p>Extract a character from reference images or videos and place them in new scenes</p></td><td><p>Spokesperson content, consistent brand characters</p></td></tr></table><p>Across the model creators, their current capabilities across the models on AI Gateway are listed below:</p><table><tr><td><p><b>Model Creator</b></p></td><td><p><b>Capabilities</b></p></td></tr><tr><td><p>xAI</p></td><td><p>Text-to-video, image-to-video, video editing, audio</p></td></tr><tr><td><p>Wan</p></td><td><p>Text-to-video, image-to-video, reference-to-video, audio</p></td></tr><tr><td><p>Kling</p></td><td><p>Text-to-video, image-to-video, first and last frame, audio</p></td></tr><tr><td><p>Veo</p></td><td><p>Text-to-video, image-to-video, audio</p></td></tr></table><h2>Text-to-video</h2><p>Describe what you want, get a video. The model handles visuals, motion, and optionally audio. Great for hyperrealistic, production-quality footage with just a simple text prompt.</p><p><b>Example: Programmatic video at scale. </b>Generate videos on demand for your app, platform, or content pipeline. No licencing fees or production required, just prompts and outputs.</p><p>This example uses <code>klingai/kling-v2.6-t2v</code> to generate video from a text prompt with a specified aspect ratio and duration.</p><div></div><p><b>Example: Creative content generation.</b> Turn a simple prompt into polished video clips for social media, ads, or storytelling with natural motion and cinematic quality.</p><p>By setting a very specific and descriptive prompt, <code>google/veo-3.1-generate-001</code> generates video with immense detail and the exact desired motion.</p><div></div><h2>Image-to-video</h2><p>Provide a starting image and animate it. Control the initial composition, then let the model generate motion.</p><p><b>Example: Animate product images.</b> Turn existing product photos into interactive videos.</p><p>The <code>klingai/kling-v2.6-i2v</code> model animates a product image after you pass an image URL and motion description in the prompt.</p><div></div><p><b>Example: Animated illustrations.</b> Bring static artwork to life with subtle motion. Perfect for thematic content or marketing at scale.</p><p><b>Example: Lifestyle and product photography.</b> Add subtle motion to food, beverage, or lifestyle shots for social content.</p><p>Here, a picture of coffee is rendered for a more interactive video, with lighting direction and minute details.</p><div></div><h2>First and last frame</h2><p>Define the start and end states, and the model generates a seamless transition between them.</p><p><b>Example: Before/after reveals.</b> Outfit swaps, product comparisons, changes over time. Upload two images, get a seamless transition.</p><p>The start and end states are defined here with two images that used in the prompt and provider options.</p><p>In this example, <code>klingai/kling-v3.0-i2v</code> lets you define the start frame in <code>image</code> and the end frame in <code>lastFrameImage</code>. The model generates the transition between them.</p><div></div><h2>Reference-to-video</h2><p>Provide reference videos or images of a person/character, and the model extracts their appearance and voice to generate new scenes starring them with consistent identity.</p><p>In this example, 2 reference images of dogs are used to generate the final video.</p><p>Using <code>alibaba/wan-v2.6-r2v-flash</code> here, you can instruct the model to utilize the people/characters within the prompt. Wan suggests using <code>character1</code>, <code>character2</code>, etc. in the prompt for multi-reference to video to get the best results.</p><div></div><h2>Video Editing</h2><p>Transform existing videos with style transfer. Provide a video URL and describe the transformation you want. The model applies the new style while preserving the original motion.</p><p>Here, <code>xai/grok-imagine-video</code> utilizes a source video from a previous generation to edit into a watercolor style.</p><div></div><h2>Get started</h2><p>For more examples and detailed configuration options for video models, check out the <a href=\"https://vercel.com/docs/ai-gateway/capabilities/video-generation\">Video Generation Documentation</a>. You can also find simple getting started scripts with the <a href=\"https://vercel.com/docs/ai-gateway/getting-started/video\">Video Generation Quick Start</a>.</p><p>Check out the changelogs for these video models for more detailed examples and prompts.</p><ul><li><p><a href=\"https://vercel.com/changelog/grok-imagine-video-on-ai-gateway\">Grok Imagine</a></p></li><li><p><a href=\"https://vercel.com/changelog/wan-models-on-ai-gateway\">Alibaba Wan</a></p></li><li><p><a href=\"https://vercel.com/changelog/veo-video-models-on-ai-gateway\">Veo</a></p></li><li><p><a href=\"https://vercel.com/changelog/kling-video-models-on-ai-gateway\">Kling</a></p></li></ul><p></p> <p class=\"more\"> <a href=\"https://vercel.com/blog/video-generation-with-ai-gateway\">Read more</a> </p>",
      "published_at": "2026-02-19T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Grok Imagine Video on AI Gateway",
      "url": "https://vercel.com/changelog/grok-imagine-video-on-ai-gateway",
      "summary": "<p>Generate high-quality videos with natural motion and audio using xAI's Grok Imagine Video, now in AI Gateway. Try it out now via the <a href=\"https://v0-grokstudio.vercel.app/\">v0 Grok Creative Studio</a>, AI SDK 6 or by selecting the model in the <a href=\"https://vercel.com/ai-gateway/models/grok-imagine-video\">AI Gateway playground</a>.</p><p>Grok Imagine is known for realistic motion and strong instruction following:</p><ul><li><p><b>Fast Generation</b>: Generates clips in seconds rather than minutes</p></li><li><p><b>Instruction Following</b>: Understands complex prompts and follow-up instructions to tweak scenes</p></li><li><p><b>Video Editing</b>: Transform existing videos by changing style, swapping objects, or altering scenes</p></li><li><p><b>Audio &amp; Dialogue</b>: Native audio generation with natural, expressive voices and accurate lip-sync</p></li></ul><h2>Three ways to get started</h2><p>Video generation is in beta and currently available for Pro and Enterprise plans and paid AI Gateway users.</p><ul><li><p><b></b><a href=\"https://v0-grokstudio.vercel.app/\"><b>v0 Grok Creative Studio</b></a>: The v0 team created a template that is powered by AI Gateway to create and showcase Grok Video and Image generations.</p></li><li><p><b>AI SDK 6</b>: Generate videos programmatically AI SDK 6's <code>generateVideo</code>.</p></li></ul><div></div><ul><li><p><b>Gateway Playground</b>: Experiment with video models with no code in the configurable <a href=\"https://vercel.com/ai-gateway/models/grok-imagine-video\">AI Gateway playground</a> that's embedded in each model page. Compare providers, tweak prompts, and download results without writing code. To access, click any video gen model in the <a href=\"https://vercel.com/ai-gateway/models?capabilities=video-generation&amp;providers=xai\">model list</a>.</p></li></ul><h2>Available Model</h2><table><tr><td><p>Model</p></td><td><p>Description</p></td></tr><tr><td><p><code>xai/grok-imagine-video</code></p></td><td><p>Text-to-video, image-to-video, and video editing</p></td></tr></table><h2>Simple: Text-to-Video</h2><p>Generate a video from a text description.</p><p>In this example, <code>xai/grok-imagine-video</code> is used to generate a video of 2 swans. Note that you can also specify the duration of the output.</p><div></div><h2>Advanced: Reference-to-Video</h2><p>Transform an existing video into a new style:</p><p>In this example, using a previous generation from Grok Imagine Video, the output was transformed into an animated watercolor style.</p><p>The source video is used and edited, which is useful for style transfer, object swapping, and scene transformations.</p><div></div><h2>Learn More</h2><p>For more examples and detailed configuration options for Grok Imagine Video, check out the <a href=\"https://vercel.com/docs/ai-gateway/capabilities/video-generation\">Video Generation Documentation</a>. You can also find simple getting started scripts with the <a href=\"https://vercel.com/docs/ai-gateway/getting-started/video\">Video Generation Quick Start</a>.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/grok-imagine-video-on-ai-gateway\">Read more</a> </p>",
      "published_at": "2026-02-19T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Wan models on AI Gateway",
      "url": "https://vercel.com/changelog/wan-models-on-ai-gateway",
      "summary": "<p>Generate stylized videos and transform existing footage with Alibaba's Wan models, now available through AI Gateway. Try them out now via AI SDK 6 or by selecting the models in the AI Gateway playground.</p><p>Wan produces artistic videos with smooth motion and can use existing content to keep videos consistent:</p><ul><li><p><b>Character Reference (R2V)</b>: Extract character appearance and voice from reference videos/images to generate new scenes</p></li><li><p><b>Flash Variants</b>: Faster generation times for quick iterations</p></li><li><p><b>Flexible Resolutions</b>: Support for 480p, 720p, and 1080p output</p></li></ul><h2>Two ways to get started</h2><p>Video generation is in beta and currently available for Pro and Enterprise plans and paid AI Gateway users.</p><ul><li><p><b>AI SDK 6</b>: Generate videos programmatically AI SDK 6's <code>generateVideo</code>.</p></li></ul><div></div><ul><li><p><b>Gateway Playground</b>: Experiment with video models with no code in the configurable <a href=\"https://vercel.com/ai-gateway/models/wan-v2.6-i2v\">AI Gateway playground</a> that's embedded in each model page. Compare providers, tweak prompts, and download results without writing code. To access, click any video gen model in the <a href=\"https://vercel.com/ai-gateway/models?capabilities=video-generation&amp;providers=alibaba\">model list</a>.</p></li></ul><h2>Available Models</h2><table><tr><td><p>Model</p></td><td><p>Type</p></td><td><p>Description</p></td></tr><tr><td><p><code>alibaba/wan-v2.6-t2v</code></p></td><td><p>Text-to-Video</p></td><td><p>Generate videos from text prompts</p></td></tr><tr><td><p><code>alibaba/wan-v2.6-i2v</code></p></td><td><p>Image-to-Video</p></td><td><p>Animate still images</p></td></tr><tr><td><p><code>alibaba/wan-v2.6-i2v-flash</code></p></td><td><p>Image-to-Video</p></td><td><p>Fast image animation</p></td></tr><tr><td><p><code>alibaba/wan-v2.6-r2v</code></p></td><td><p>Reference-to-Video</p></td><td><p>Character transfer from references</p></td></tr><tr><td><p><code>alibaba/wan-v2.6-r2v-flash</code></p></td><td><p>Reference-to-Video</p></td><td><p>Fast style transfer</p></td></tr><tr><td><p><code>alibaba/wan-v2.5-t2v-preview</code></p></td><td><p>Text-to-Video</p></td><td><p>Previous version</p></td></tr></table><h2>Simple: Text-to-Video with Audio</h2><p>Generate a stylized video from a text description.</p><p>You can use detailed prompts and specify styles with the Wan models to achieve the desired output generation. The example here uses <code>alibaba/wan-v2.6-t2v</code>:</p><div></div><h2>Advanced: Reference-to-Video</h2><p>Generate new scenes using characters extracted from reference images or videos.</p><p>In this example, 2 reference images of dogs are used to generate the final video.</p><p>Using <code>alibaba/wan-v2.6-r2v-flash</code> here, you can instruct the model to utilize the people/characters within the prompt. Wan suggests using <code>character1</code>, <code>character2</code>, etc. in the prompt for multi-reference to video to get the best results.</p><div></div><h2>Learn More</h2><p>For more examples and detailed configuration options for Wan models, check out the <a href=\"https://vercel.com/docs/ai-gateway/capabilities/video-generation\">Video Generation Documentation</a>. You can also find simple getting started scripts with the <a href=\"https://vercel.com/docs/ai-gateway/getting-started/video\">Video Generation Quick Start</a>.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/wan-models-on-ai-gateway\">Read more</a> </p>",
      "published_at": "2026-02-19T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Kling video models on AI Gateway",
      "url": "https://vercel.com/changelog/kling-video-models-on-ai-gateway",
      "summary": "<p>Kling video models are now available in AI Gateway, including the newest Kling 3.0 models. Generate cinematic videos from text, images, or motion references with Kling's state-of-the-art video models, now available through AI Gateway and AI SDK.</p><p>Kling models are known for their image to video models and multishot capabilities:</p><ul><li><p><b>Image-to-Video Capabilities</b>: Strong at animating still images into video clips</p></li><li><p><b>Realistic Motion and Physics</b>: Known for coherent motion, facial expressions, and physical interactions</p></li><li><p><b>High Resolution Output</b>: Supports up to 1080p generation (pro mode)</p></li><li><p><b>Multishot Narratives</b>: Kling 3.0 can generate multi-scene videos from a single narrative prompt</p></li><li><p><b>Audio Generation</b>: Create synchronized sound effects and ambient audio alongside your video</p></li><li><p><b>First &amp; Last Frame Control</b>: Specify both start and end frames for precise scene transitions</p></li></ul><h2>Two ways to get started</h2><p>Video generation is in beta and currently available for Pro and Enterprise plans and paid AI Gateway users.</p><ul><li><p><b>AI SDK 6</b>: Generate videos programmatically AI SDK 6's <code>generateVideo</code>. </p></li></ul><div></div><ul><li><p><b>Gateway Playground</b>: Experiment with video models with no code in the configurable <a href=\"https://vercel.com/ai-gateway/models/kling-v3.0-t2v\">AI Gateway playground</a> that's embedded in each model page. Compare providers, tweak prompts, and download results without writing code. To access, click any video gen model in the <a href=\"https://vercel.com/ai-gateway/models?capabilities=video-generation&amp;providers=klingai\">model list</a>.</p></li></ul><h2>Available Models</h2><table><tr><td><p>Model</p></td><td><p>Type</p></td><td><p>Description</p></td></tr><tr><td><p><code>klingai/kling-v3.0-t2v</code></p></td><td><p>Text-to-Video</p></td><td><p>Latest generation, highest quality with multishot support</p></td></tr><tr><td><p><code>klingai/kling-v3.0-i2v</code></p></td><td><p>Image-to-Video, First-and-Last-Frame</p></td><td><p>Animate images with v3 quality and multiple frames</p></td></tr><tr><td><p><code>klingai/kling-v2.6-t2v</code></p></td><td><p>Text-to-Video</p></td><td><p>Audio generation support</p></td></tr><tr><td><p><code>klingai/kling-v2.6-i2v</code></p></td><td><p>Image-to-Video, First-and-Last-Frame</p></td><td><p>Use images as reference</p></td></tr><tr><td><p><code>klingai/kling-v2.5-turbo-t2v</code></p></td><td><p>Text-to-Video</p></td><td><p>Faster generation</p></td></tr><tr><td><p><code>klingai/kling-v2.5-turbo-i2v</code></p></td><td><p>Image-to-Video, First-and-Last-Frame</p></td><td><p>Faster generation</p></td></tr></table><h2>Simple: Text-to-Video with Audio</h2><p>Generate a video from a text description.</p><p>In this example, model <code>klingai/kling-v3.0-t2v</code> is used to generate a video of a cherry blossom tree with no inputs other than a simple text prompt.</p><div></div><h2>Advanced: Multishot Video</h2><p>Generate a narrative video with multiple scenes with only a single prompt. Using Kling 3.0's multishot feature, the model intelligently cuts between shots to tell a complete story:</p><p>The prompt is written as a narrative with multiple distinct scenes for the best results. <code>shotType: 'intelligence'</code> lets the model decide optimal shot composition and <code>sound: 'on'</code> generates synchronized audio for the entire video. Note that the prompt here is in the <code>providerOptions</code> since this functionality is specific to Kling. The Kling 3.0 models support this functionality: here <code>klingai/kling-v3.0-t2v</code> is used.</p><div></div><h2>Advanced: First &amp; Last Frame Control</h2><p>Control exactly how your video starts and ends by providing both a first frame and last frame image. This is perfect for time-lapse effects or precise scene transitions:</p><p>These 2 images were provided as start and end frames.</p><p>Using AI SDK 6, you can set <code>image</code> and <code>lastFrameImage</code> with your start and end frames. In this example, <code>klingai/kling-v3.0-i2v</code> is used for the model. </p><div></div><h2>Learn More</h2><p>For more examples and detailed configuration options for Kling models, check out the <a href=\"https://vercel.com/docs/ai-gateway/capabilities/video-generation\">Video Generation Documentation</a>. You can also find simple getting started scripts with the <a href=\"https://vercel.com/docs/ai-gateway/getting-started/video\">Video Generation Quick Start</a>.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/kling-video-models-on-ai-gateway\">Read more</a> </p>",
      "published_at": "2026-02-19T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Veo video models on AI Gateway",
      "url": "https://vercel.com/changelog/veo-video-models-on-ai-gateway",
      "summary": "<p>Generate photorealistic videos with synchronized audio using Google's Veo models, now available through AI Gateway. Try them out now via AI SDK 6 or by selecting the models in the AI Gateway playground.</p><p>Veo models are known for their cinematic quality and audio generation:</p><ul><li><p><b>Native Audio Generation: </b>Automatically generate realistic sound effects, ambient audio, and even dialogue that matches your video</p></li><li><p><b>Up to 1080p Resolution: </b>Generate videos at 720p and 1080p</p></li><li><p><b>Photorealistic Quality: </b>Realism for nature, wildlife, and cinematic scenes</p></li><li><p><b>Image-to-Video: </b>Animate still photos with natural motion</p></li><li><p><b>Fast Mode: </b>Quicker generation when you need rapid iterations</p></li></ul><h2>Two ways to get started</h2><p>Video generation is in beta and currently available for Pro and Enterprise plans and paid AI Gateway users.</p><ul><li><p><b>AI SDK 6</b>: Generate videos programmatically AI SDK 6's <code>generateVideo</code>. </p></li></ul><div></div><ul><li><p><b>Gateway Playground</b>: Experiment with video models with no code in the configurable <a href=\"https://vercel.com/ai-gateway/models/veo-3.1-generate-001\">AI Gateway playground</a> that's embedded in each model page. Compare providers, tweak prompts, and download results without writing code. To access, click any video gen model in the <a href=\"https://vercel.com/ai-gateway/models?capabilities=video-generation&amp;providers=vertex\">model list</a>.</p></li></ul><h2>Available Models</h2><table><tr><td><p>Model</p></td><td><p>Description</p></td></tr><tr><td><p><code>google/veo-3.1-generate-001</code></p></td><td><p>Latest generation, highest quality</p></td></tr><tr><td><p><code>google/veo-3.1-fast-generate-001</code></p></td><td><p>Fast mode for quicker iterations</p></td></tr><tr><td><p><code>google/veo-3.0-generate-001</code></p></td><td><p>Full quality generation</p></td></tr><tr><td><p><code>google/veo-3.0-fast-generate-001</code></p></td><td><p>Fast mode generation</p></td></tr></table><h2>Simple: Text-to-Video with Audio</h2><p>Describe a scene and get a video.</p><p>Generate a cinematic wildlife video with natural sound: here <code>google/veo-3.1-generate-001</code> is used with <code>generateAudio: true</code>.</p><div></div><h2>Advanced: Image-to-Video with Dialog</h2><p>A common workflow to ensure quality is generating a custom image with Gemini 3 Pro Image (Nano Banana Pro), then bringing it to life with Veo, complete with motion and spoken dialog.</p><p>Starting image from Nano Banana Pro:</p><p>Use prompts with image input with the Veo models for more control over the output. This example uses <code>google/veo-3.1-generate-001</code>, which supports image to video. </p><div></div><h2>Learn More</h2><p>For more examples and detailed configuration options for Veo models, check out the <a href=\"https://vercel.com/docs/ai-gateway/capabilities/video-generation\">Video Generation Documentation</a>. You can also find simple getting started scripts with the <a href=\"https://vercel.com/docs/ai-gateway/getting-started/video\">Video Generation Quick Start</a>.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/veo-video-models-on-ai-gateway\">Read more</a> </p>",
      "published_at": "2026-02-19T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Access billing usage and cost data via API",
      "url": "https://vercel.com/changelog/access-billing-usage-cost-data-api",
      "summary": "<p>Vercel now supports programmatic access to billing usage and cost data through the API and CLI. The new <code>/billing/charges</code> endpoint returns data in the <a href=\"https://focus.finops.org/\">FOCUS v1.3 open-standard format</a>, allowing teams to ingest cost data into FinOps tools without custom transformation logic.</p><p>The endpoint supports 1-day granularity with a maximum date range of one year. Responses are streamed as newline-delimited JSON (JSONL) to handle large datasets efficiently.</p><p><b>SDK usage</b></p><div></div><p><code><b>curl</b></code><b> usage</b></p><div></div><p><b>CLI usage</b></p><p>For quick introspection, the <code>vercel usage</code> command displays billing usage for the current period or a custom date range. This includes credit-use and costs for each service.</p><p><b>View usage for the current billing period</b></p><div></div><p><b>View usage for a custom date range</b></p><div></div><p>Vantage has also released a native integration that connects Vercel teams to Vantage accounts. This automatically syncs usage and cost data alongside other tools, simplifying cost observability. Read the <a href=\"https://www.vantage.sh/blog/vercel-cost-management\">Vantage announcement blog</a> for details.</p><p>Learn more in the <a href=\"https://docs.vercel.com/docs/rest-api/reference/endpoints/billing/list-focus-billing-charges\">API documentation</a> and <a href=\"https://vercel.com/docs/cli/usage\">CLI reference</a>.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/access-billing-usage-cost-data-api\">Read more</a> </p>",
      "published_at": "2026-02-19T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Streamdown 2.3 — Refreshed design and interactive playground",
      "url": "https://vercel.com/changelog/streamdown-2-3",
      "summary": "<p>Streamdown 2.3 enhances design consistency by applying a unified nested-card design to tables, code blocks, and Mermaid diagrams. Action buttons now remain sticky during scroll, and code blocks render plain text immediately to reduce perceived latency before syntax highlighting loads.</p><p>To accelerate testing, the new interactive playground supports real-time execution with custom markdown and editable props. This enables faster experimentation with configuration changes without spinning up a local project.</p><p>New hooks and utilities provide improved control over rendering. The <code>useIsCodeFenceIncomplete</code> hook detects in-progress fenced code blocks during streaming. Tables now support copying as Markdown, and a new HTML indentation normalization property handles inconsistent whitespace in raw input. Image rendering also includes improved error handling with custom messaging.</p><p>Documentation has been reorganized for easier reference. Plugin documentation for CJK, Math, and Mermaid is now consolidated into dedicated pages, and the redesigned homepage links directly to templates for faster onboarding.</p><p>This release also resolves issues with nested HTML block parsing, custom tag handling, Mermaid diagram artifacts, and Shiki syntax engine inconsistencies. Streamdown 2.3 ships with a fully cleared bug backlog.</p><p><a href=\"https://streamdown.ai/docs\">Read the documentation</a> for more information.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/streamdown-2-3\">Read more</a> </p>",
      "published_at": "2026-02-19T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Gemini 3.1 Pro is live on AI Gateway",
      "url": "https://vercel.com/changelog/gemini-3-1-pro-is-live-on-ai-gateway",
      "summary": "<p>Gemini 3.1 Pro Preview from Google is now available on AI Gateway.</p><p>This model release brings quality improvements across software engineering and agentic workflows, with enhanced usability for real-world tasks in finance and spreadsheet applications. Gemini 3.1 Pro Preview introduces more efficient thinking across use cases, reducing token consumption while maintaining performance.</p><p>To use this model, set model to <code>google/gemini-3.1-pro-preview</code> in the AI SDK. This model supports the <code>medium</code> thinking level for finer control over the trade-offs between cost, performance, and speed.</p><div></div><p>AI Gateway provides a unified API for calling models, tracking usage and cost, and configuring retries, failover, and performance optimizations for higher-than-provider uptime. It includes built-in <a href=\"https://vercel.com/docs/observability/ai-sdk-observability\">observability</a>, <a href=\"https://vercel.com/docs/ai-gateway#bring-your-own-key\">Bring Your Own Key</a> support, and intelligent provider routing with automatic retries.</p><p>Learn more about <a href=\"https://vercel.com/ai-gateway\">AI Gateway</a>, view the <a href=\"https://vercel.com/ai-gateway/leaderboards\">AI Gateway model leaderboard</a> or try it in our <a href=\"https://vercel.com/ai-gateway/models/gemini-3.1-pro-preview\">model playground</a>.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/gemini-3-1-pro-is-live-on-ai-gateway\">Read more</a> </p>",
      "published_at": "2026-02-19T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Private storage for Vercel Blob, now available in public beta",
      "url": "https://vercel.com/changelog/private-storage-for-vercel-blob-now-available-in-public-beta",
      "summary": "<p>Vercel Blob now supports private storage for sensitive files like contracts, invoices, and internal reports. Private storage requires authentication for all operations, preventing exposure via public URLs.</p><p>Public storage allows public reads for media assets, while private storage requires authentication.</p><p>Create a private store via the <a href=\"https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fstores&amp;title=Vercel+Storage\">Storage dashboard</a> or with the CLI:</p><p><b>CLI command</b></p><div></div><p>When created inside a linked Vercel project, the CLI prompts you to connect the store, automatically adding the <code>BLOB_READ_WRITE_TOKEN</code> environment variable. The SDK uses this variable to authenticate operations in your deployments.</p><p><b>SDK installation</b></p><div></div><p>To upload, use <code>put</code> or <code>upload</code> with the <code>access: 'private'</code> option.</p><p><b>Upload example</b></p><div></div><p>To download, use the <code>get</code> method to stream files.</p><p><b>Retrieval example</b></p><div></div><p>Private storage is in beta on all plans with standard <a href=\"https://vercel.com/docs/storage/vercel-blob/usage-and-pricing\">Vercel Blob pricing</a>.</p><p><a href=\"https://vercel.com/docs/storage/vercel-blob/private-storage\">Learn more about private storage</a>.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/private-storage-for-vercel-blob-now-available-in-public-beta\">Read more</a> </p>",
      "published_at": "2026-02-19T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "We Ralph Wiggumed WebStreams to make them 10x faster",
      "url": "https://vercel.com/blog/we-ralph-wiggumed-webstreams-to-make-them-10x-faster",
      "summary": "<p>When we started profiling Next.js server rendering earlier this year, one thing kept showing up in the flamegraphs: WebStreams. Not the application code running inside them, but the streams themselves. The Promise chains, the per-chunk object allocations, the microtask queue hops. After <a href=\"https://vercel.com/blog/fluid-compute-benchmark-results\">Theo Browne's server rendering benchmarks</a> highlighted how much compute time goes into framework overhead, we started looking at where that time actually goes. A lot of it was in streams.</p><p>Turns out that WebStreams have an <a href=\"https://github.com/web-platform-tests/wpt/tree/master/streams\">incredibly complete test suite</a>, and that makes them a great candidate for doing an AI-based re-implementation in a purely test-driven and benchmark-driven fashion. This post is about the performance work we did, what we learned, and how this work is already making its way into Node.js itself through Matteo Collina's <a href=\"https://github.com/nodejs/node/pull/61807\">upstream PR</a>.</p><h2>The problem</h2><p>Node.js has two streaming APIs. The older one (<code>stream.Readable</code>, <code>stream.Writable</code>, <code>stream.Transform</code>) has been around for over a decade and is heavily optimized. Data moves through C++ internals. Backpressure is a boolean. Piping is a single function call.</p><p>The newer one is the WHATWG Streams API: <code>ReadableStream</code>, <code>WritableStream</code>, <code>TransformStream</code>. This is the web standard. It powers <code>fetch()</code> response bodies, <code>CompressionStream</code>, <code>TextDecoderStream</code>, and increasingly, server-side rendering in frameworks like Next.js and React.</p><p>The web standard is the right API to converge on. But on the server, it is slower than it needs to be.</p><p>To understand why, consider what happens when you call <code>reader.read()</code> on a native WebStream in Node.js. Even if data is already sitting in the buffer:</p><ol><li><p>A <code>ReadableStreamDefaultReadRequest</code> object is allocated with three callback slots</p></li><li><p>The request is enqueued into the stream's internal queue</p></li><li><p>A new Promise is allocated and returned</p></li><li><p>Resolution goes through the microtask queue</p></li></ol><p>That is four allocations and a microtask hop to return data that was already there. Now multiply that by every chunk flowing through every transform in a rendering pipeline.</p><p>Or consider <code>pipeTo()</code>. Each chunk passes through a full Promise chain: read, write, check backpressure, repeat. An <code>{value, done}</code> result object is allocated per read. Error propagation creates additional Promise branches.</p><p>None of this is wrong. These guarantees matter in the browser where streams cross security boundaries, where cancellation semantics need to be airtight, where you do not control both ends of a pipe. But on the server, when you are piping React Server Components through three transforms at 1KB chunks, the cost adds up.</p><p>We benchmarked native WebStream <code>pipeThrough</code> at <b>630 MB/s</b> for 1KB chunks. Node.js <code>pipeline()</code> with the same passthrough transform: <b>~7,900 MB/s</b>. That is a 12x gap, and the difference is almost entirely Promise and object allocation overhead.</p><h2>What we built</h2><p>We have been working on a library called <code>fast-webstreams</code> that implements the WHATWG <code>ReadableStream</code>, <code>WritableStream</code>, and <code>TransformStream</code> APIs backed by Node.js streams internally. Same API, same error propagation, same spec compliance. The overhead is removed for the common cases.</p><p>The core idea is to route operations through different fast paths depending on what you are actually doing:</p><h3>When you pipe between fast streams: zero Promises</h3><p>This is the biggest win. When you chain <code>pipeThrough</code> and <code>pipeTo</code> between fast streams, the library does not start piping immediately. Instead, it records upstream links:</p><p><code>source → transform1 → transform2 → ...</code></p><p>When <code>pipeTo()</code> is called at the end of the chain, it walks upstream, collects the underlying Node.js stream objects, and issues a single <code>pipeline()</code> call. One function call. Zero Promises per chunk. Data flows through Node's optimized C++ path.</p><div></div><p>The result: <b>~6,200 MB/s</b>. That is ~10x faster than native WebStreams and close to raw Node.js pipeline performance.</p><p>If any stream in the chain is not a fast stream (say, a native <code>CompressionStream</code>), the library falls back to either native <code>pipeThrough</code> or a spec-compliant <code>pipeTo</code> implementation.</p><h3>When you read chunk by chunk: synchronous resolution</h3><p>When you call <code>reader.read()</code>, the library tries <code>nodeReadable.read()</code> synchronously. If data is there, you get <code>Promise.resolve({value, done})</code>. No event loop round-trip. No request object allocation. Only when the buffer is empty does it register a listener and return a pending Promise.</p><div></div><p>The result: <b>~12,400 MB/s</b>, or 3.7x faster than native.</p><h3>The React Flight pattern: where the gap is largest</h3><p>This is the one that matters most for Next.js. React Server Components use a specific byte stream pattern: create a <code>ReadableStream</code> with <code>type: 'bytes'</code>, capture the controller in <code>start()</code>, enqueue chunks externally as the render produces them.</p><div></div><p>Native WebStreams: <b>~110 MB/s</b>. fast-webstreams: <b>~1,600 MB/s</b>. That is <b>14.6x faster</b> for the exact pattern used in production server rendering.</p><p>The speed comes from <code>LiteReadable</code>, a minimal array-based buffer we wrote to replace Node.js's <code>Readable</code> for byte streams. It uses direct callback dispatch instead of EventEmitter, supports pull-based demand and BYOB readers, and costs about 5 microseconds less per construction. That matters when React Flight creates hundreds of byte streams per request.</p><h3>Fetch response bodies: streams you don't construct yourself</h3><p>The examples above all start with <code>new ReadableStream(...)</code>. But on the server, most streams do not start that way. They start from <code>fetch()</code>. The response body is a native byte stream owned by Node.js's HTTP layer. You cannot swap it out.</p><p>This is a common pattern in server-side rendering: fetch data from an upstream service, pipe the response through one or more transforms, and forward the result to the client.</p><div></div><p>With native WebStreams, each hop in this chain pays the full Promise-per-chunk cost. Three transforms means roughly 6-9 Promises per chunk. At 1KB chunks, that gets you <b>~260 MB/s</b>.</p><p>The library handles this through deferred resolution. When <code>patchGlobalWebStreams()</code> is active, <code>Response.prototype.body</code> returns a lightweight fast shell wrapping the native byte stream. Calling <code>pipeThrough()</code> does not start piping immediately. It just records the link. Only when <code>pipeTo()</code> or <code>getReader()</code> is called at the end does the library resolve the full chain: it creates a single bridge from the native reader into Node.js <code>pipeline()</code> for the transform hops, then serves reads from the buffered output synchronously.</p><p>The cost model: one Promise at the native boundary to pull data in. Zero Promises through the transform chain. Sync reads at the output.</p><p>The result: <b>~830 MB/s</b>, or <b>3.2x faster</b> than native for the three-transform fetch pattern. For simple response forwarding without transforms, it is <b>2.0x faster</b> (850 vs 430 MB/s).</p><h2>Benchmarks</h2><p>All numbers are throughput in MB/s at 1KB chunks on Node.js v22. Higher is better.</p><h3>Core operations</h3><table><tr><td><p>Operation</p></td><td><p>Node.js streams</p></td><td><p>fast</p></td><td><p>native</p></td><td><p>fast vs native</p></td></tr><tr><td><p>read loop</p></td><td><p>26,400</p></td><td><p><b>12,400</b></p></td><td><p>3,300</p></td><td><p><b>3.7x</b></p></td></tr><tr><td><p>write loop</p></td><td><p>26,500</p></td><td><p><b>5,500</b></p></td><td><p>2,300</p></td><td><p><b>2.4x</b></p></td></tr><tr><td><p>pipeThrough</p></td><td><p>7,900</p></td><td><p><b>6,200</b></p></td><td><p>630</p></td><td><p><b>9.8x</b></p></td></tr><tr><td><p>pipeTo</p></td><td><p>14,000</p></td><td><p><b>2,500</b></p></td><td><p>1,400</p></td><td><p><b>1.8x</b></p></td></tr><tr><td><p>for-await-of</p></td><td><p>—</p></td><td><p><b>4,100</b></p></td><td><p>3,000</p></td><td><p><b>1.4x</b></p></td></tr></table><h3>Transform chains</h3><p>The Promise-per-chunk overhead compounds with chain depth:</p><table><tr><td><p>Depth</p></td><td><p>fast</p></td><td><p>native</p></td><td><p>fast vs native</p></td></tr><tr><td><p>3 transforms</p></td><td><p><b>2,900</b></p></td><td><p>300</p></td><td><p><b>9.7x</b></p></td></tr><tr><td><p>8 transforms</p></td><td><p><b>1,000</b></p></td><td><p>115</p></td><td><p><b>8.7x</b></p></td></tr></table><h3>Byte streams</h3><table><tr><td><p>Pattern</p></td><td><p>fast</p></td><td><p>native</p></td><td><p>fast vs native</p></td></tr><tr><td><p>start + enqueue (React Flight)</p></td><td><p><b>1,600</b></p></td><td><p>110</p></td><td><p><b>14.6x</b></p></td></tr><tr><td><p>byte read loop</p></td><td><p><b>1,400</b></p></td><td><p>1,400</p></td><td><p>1.0x</p></td></tr><tr><td><p>byte tee</p></td><td><p><b>1,200</b></p></td><td><p>750</p></td><td><p><b>1.6x</b></p></td></tr></table><h3>Response body patterns</h3><table><tr><td><p>Pattern</p></td><td><p>fast</p></td><td><p>native</p></td><td><p>fast vs native</p></td></tr><tr><td><p>Response.text()</p></td><td><p>900</p></td><td><p>910</p></td><td><p>1.0x</p></td></tr><tr><td><p>Response forwarding</p></td><td><p><b>850</b></p></td><td><p>430</p></td><td><p><b>2.0x</b></p></td></tr><tr><td><p>fetch → 3 transforms</p></td><td><p><b>830</b></p></td><td><p>260</p></td><td><p><b>3.2x</b></p></td></tr></table><h3>Stream construction</h3><p>Creating streams is also faster, which matters for short-lived streams:</p><table><tr><td><p>Type</p></td><td><p>fast</p></td><td><p>native</p></td><td><p>fast vs native</p></td></tr><tr><td><p>ReadableStream</p></td><td><p><b>2,100</b></p></td><td><p>980</p></td><td><p><b>2.1x</b></p></td></tr><tr><td><p>WritableStream</p></td><td><p><b>1,300</b></p></td><td><p>440</p></td><td><p><b>3.0x</b></p></td></tr><tr><td><p>TransformStream</p></td><td><p><b>470</b></p></td><td><p>220</p></td><td><p><b>2.1x</b></p></td></tr></table><h2>Spec compliance</h2><p>fast-webstreams passes <b>1,100 out of 1,116</b> Web Platform Tests. Node.js's native implementation passes 1,099. The 16 failures that remain are either shared with native (like the unimplemented <code>type: 'owning'</code> transfer mode) or are architectural differences that do not affect real applications.</p><h2>How we are deploying this</h2><p>The library can patch the global <code>ReadableStream</code>, <code>WritableStream</code>, and <code>TransformStream</code> constructors:</p><div></div><p>The patch also intercepts <code>Response.prototype.body</code> to wrap native fetch response bodies in fast stream shells, so <code>fetch()</code> → <code>pipeThrough()</code> → <code>pipeTo()</code> chains hit the pipeline fast path automatically.</p><p>At Vercel, we are looking at rolling this out across our fleet. We will do so carefully and incrementally. Streaming primitives sit at the foundation of request handling, response rendering, and compression. We are starting with the patterns where the gap is largest: React Server Component streaming, response body forwarding, and multi-transform chains. We will measure in production before expanding further.</p><h2>The right fix is upstream</h2><p>A userland library should not be the long-term answer here. The right fix is in Node.js itself.</p><p>Work is already happening. After a conversation on X, Matteo Collina submitted <a href=\"https://github.com/nodejs/node/pull/61807\">nodejs/node#61807</a>, \"stream: add fast paths for webstreams read and pipeTo.\" The PR applies two ideas from this project directly to Node.js's native WebStreams:</p><ol><li><p><b><code>read()</code></b><b> fast path</b>: When data is already buffered, return a resolved Promise directly without creating a </p><p><code>ReadableStreamDefaultReadRequest</code> object. This is spec-compliant because <code>read()</code> returns a Promise either way, and resolved promises still run callbacks in the microtask queue.</p></li><li><p><b><code>pipeTo()</code></b><b> batch reads</b>: When data is buffered, batch multiple reads from the controller queue without creating per-chunk request objects. Backpressure is respected by checking <code>desiredSize</code> after each write.</p></li></ol><p>The PR shows <b>~17-20% faster</b> buffered reads and <b>~11% faster</b> <code>pipeTo</code>. These improvements benefit every Node.js user for free. No library to install, no patching, no risk.</p><p>James Snell's <a href=\"https://github.com/nodejs/performance/issues/134\">Node.js performance issue #134</a> outlines several additional opportunities: C++-level piping for internally-sourced streams, lazy buffering, eliminating double-buffering in WritableStream adapters. Each of these could close the gap further.</p><p>We will keep contributing ideas upstream. The goal is not for fast-webstreams to exist forever. The goal is for WebStreams to be fast enough that it does not need to.</p><h2>What we learned the hard way</h2><p><b>The spec is smarter than it looks.</b> We tried many shortcuts. Almost every one of them broke a Web Platform Test, and the test was usually right. The <code>ReadableStreamDefaultReadRequest</code> pattern, the Promise-per-read design, the careful error propagation: they exist because cancellation during reads, error identity through locked streams, and thenable interception are real edge cases that real code hits.</p><p><b><code>Promise.resolve(obj)</code></b><b> always checks for thenables.</b> This is a language-level behavior you cannot avoid. If the object you resolve with has a <code>.then</code> property, the Promise machinery will call it. Some WPT tests deliberately put <code>.then</code> on read results and verify that the stream handles it correctly. We had to be very careful about where <code>{value, done}</code> objects get created in hot paths.</p><p><b>Node.js </b><b><code>pipeline()</code></b><b> cannot replace WHATWG </b><b><code>pipeTo</code></b><b>.</b> We hoped to use <code>pipeline()</code> for all piping. It causes 72 WPT failures. The error propagation, stream locking, and cancellation semantics are fundamentally different. <code>pipeline()</code> is only safe when we control the entire chain, which is why we collect upstream links and only use it for full fast-stream chains.</p><p><b><code>Reflect.apply</code></b><b>, not </b><b><code>.call()</code></b><b>.</b> The WPT suite monkey-patches <code>Function.prototype.call</code> and verifies that implementations do not use it to invoke user-provided callbacks. <code>Reflect.apply</code> is the only safe way. This is a real spec requirement.</p><h2>We built most of fast-webstreams with AI</h2><p>Two things made that viable:</p><p>The amazing <a href=\"https://web-platform-tests.org/\">Web Platform Tests</a> gave us 1,116 tests as an immediate, machine-checkable answer to \"did we break anything?\" And we built a benchmark suite early on so we could measure whether each change actually moved throughput. The development loop was: implement an optimization, run the WPT suite, run benchmarks. When tests broke, we knew which spec invariant we had violated. When benchmarks did not move, we reverted.</p><p>The WHATWG Streams spec is long and dense. The interesting optimization opportunities sit in the gap between what the spec <i>requires</i> and what current implementations <i>do</i>. <code>read()</code> must return a Promise, but nothing says that Promise cannot already be resolved when data is buffered. That kind of observation is straightforward when you can ask an AI to analyze algorithm steps for places where the observable behavior can be preserved with fewer allocations.</p><h2>Try it</h2><p>fast-webstreams is <a href=\"https://www.npmjs.com/package/experimental-fast-webstreams\">available on npm</a> as <code>experimental-fast-webstreams</code>. The \"experimental\" prefix is intentional. We are confident in correctness, but this is an area of active development.</p><p>If you are building a server-side JavaScript framework or runtime and hitting WebStreams performance limits, we would love to hear from you. And if you are interested in improving WebStreams in Node.js itself, Matteo's <a href=\"https://github.com/nodejs/node/pull/61807\">PR</a> is a great place to start.</p> <p class=\"more\"> <a href=\"https://vercel.com/blog/we-ralph-wiggumed-webstreams-to-make-them-10x-faster\">Read more</a> </p>",
      "published_at": "2026-02-18T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Redesigned search and filtering for runtime logs",
      "url": "https://vercel.com/changelog/redesigned-search-and-filtering-for-runtime-logs",
      "summary": "<div></div><p>The Runtime Logs search bar in your project dashboard has been redesigned to make filtering and exploring your logs faster and more intuitive.</p><ul><li><p><b>Structured filters.</b> When you type a filter like <code>level:error</code> or <code>status:500</code>, the search bar parses it into a visual pill you can read at a glance and remove with a click. Complex queries with multiple filters become easy to scan and edit without retyping anything</p></li><li><p><b>Smarter suggestions.</b> As you type, the search bar suggests filter values based on your actual log data. Recent queries are saved per-project and appear at the top, so you can rerun common searches without retyping them</p></li><li><p><b>Better input handling.</b> The search bar validates your filters as you type and flags errors with a tooltip so you can fix typos before running a search. Pasting a Vercel Request ID automatically converts it into a filter</p></li></ul><p>These improvements are available now in your project dashboard. Learn more about <a href=\"https://vercel.com/docs/observability/runtime-logs\">runtime logs</a>.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/redesigned-search-and-filtering-for-runtime-logs\">Read more</a> </p>",
      "published_at": "2026-02-18T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "How Stably ships AI testing agents in hours, not weeks",
      "url": "https://vercel.com/blog/How-Stably-ships-AI-testing-agents-in-hours-not-weeks",
      "summary": "<p>How the 6-person team at Stably ships AI testing agents faster with Vercel—moving from weeks to hours. Their shift highlights how Vercel's platform eliminates infrastructure anxiety, boosting autonomous testing and enabling quick enterprise growth.</p> <p class=\"more\"> <a href=\"https://vercel.com/blog/How-Stably-ships-AI-testing-agents-in-hours-not-weeks\">Read more</a> </p>",
      "published_at": "2026-02-17T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Automatic build fix suggestions with Vercel Agent",
      "url": "https://vercel.com/changelog/automatic-build-fix-suggestions-with-vercel-agent",
      "summary": "<p>You can now get automatic code-fix suggestions for broken builds from the Vercel Agent, directly in GitHub pull request reviews or in the Vercel Dashboard.</p><p>When the Vercel Agent reviews your pull request, it now scans your deployments for build errors, and when it detects failures it automatically suggests a code fix based on your code and build logs.</p><div></div><p>In addition, Vercel Agent can automatically suggest code fixes inside the Vercel dashboard whenever a build error is detected, and suggests a code change to a GitHub Pull Request for review before merging with your code.</p><div></div><p>Get started with Vercel Agent code review in the <a href=\"https://vercel.com/~/agent\">Agent dashboard</a>, or learn more in the <a href=\"https://vercel.com/docs/agent\">documentation</a>.</p><p></p><p></p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/automatic-build-fix-suggestions-with-vercel-agent\">Read more</a> </p>",
      "published_at": "2026-02-17T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Automated security audits now available for skills.sh",
      "url": "https://vercel.com/changelog/automated-security-audits-now-available-for-skills-sh",
      "summary": "<p>Skills on the <a href=\"http://skills.sh\">skills.sh</a> now have automated security audits to help developers use skills with confidence.</p><p>Working with our partners <a href=\"https://ai.gendigital.com/\">Gen</a>, <a href=\"https://socket.dev/\">Socket</a>, and <a href=\"https://snyk.io/\">Snyk</a>, these independent security reports allow us to rapidly scale and audit over 60,000 skills and counting.</p><p><a href=\"http://skills.sh/\">Skills.sh</a> provides greater ecosystem support with:</p><ul><li><p>Transparent results: Security audits appear publicly on each skill's detail page.</p></li><li><p>Leaderboard protection : Skills flagged as malicious are automatically hidden from the leaderboard and search results. If you navigate directly to a flagged skill, a warning note appears before installation.</p></li><li><p>Security validation: As of <code>skills@1.4.0</code>, adding skills clearly displays audit results and risk levels before installation.</p></li></ul><p>Learn more at <a href=\"http://skills.sh\">skills.sh</a>.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/automated-security-audits-now-available-for-skills-sh\">Read more</a> </p>",
      "published_at": "2026-02-17T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Recraft V4 on AI Gateway",
      "url": "https://vercel.com/changelog/recraft-v4-on-ai-gateway",
      "summary": "<p>Recraft V4 is now available on AI Gateway.</p><p>A text-to-image model built for professional design and marketing use cases, V4 was developed with input from working designers. The model has improvements with photorealism, with realistic skin, natural textures, and fewer synthetic artifacts. It also produces images with clean lighting and varied composition. For illustration, the model can generate original characters with less predictable color palettes.</p><p>There are 2 versions:</p><ul><li><p><b>V4</b>: Faster and more cost-efficient, suited for everyday work and iteration</p></li><li><p><b>V4 Pro</b>: Generates higher-resolution images for print-ready assets and large-scale use</p></li></ul><p>To use this model, set model to <code>recraft/recraft-v4-pro</code> or <code>recraft/recraft-v4</code> in the AI SDK:</p><div></div><p>AI Gateway provides a unified API for calling models, tracking usage and cost, and configuring retries, failover, and performance optimizations for higher-than-provider uptime. It includes built-in <a href=\"https://vercel.com/docs/observability/ai-sdk-observability\">observability</a>, <a href=\"https://vercel.com/docs/ai-gateway#bring-your-own-key\">Bring Your Own Key</a> support, and intelligent provider routing with automatic retries.</p><p>Learn more about <a href=\"https://vercel.com/ai-gateway\">AI Gateway</a>, view the <a href=\"https://vercel.com/ai-gateway/leaderboards\">AI Gateway model leaderboard</a> or try it in our <a href=\"https://vercel.com/ai-gateway/models/recraft-v4\">model playground</a>.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/recraft-v4-on-ai-gateway\">Read more</a> </p>",
      "published_at": "2026-02-17T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Vercel Sandbox snapshots now allow custom retention periods",
      "url": "https://vercel.com/changelog/vercel-sandbox-snapshots-now-allow-custom-retention-periods",
      "summary": "<p>Snapshots created with <a href=\"https://vercel.com/docs/vercel-sandbox\">Vercel Sandbox</a> now have configurable expiration, instead of the previous 7 days limit, along with higher defaults.</p><div></div><p>The expiration can be configured between <a href=\"/docs/vercel-sandbox/sdk-reference#sandbox.snapshot\">1 day to infinity.</a> If not provided, the default snapshot expiration is 30 days.</p><p>You can also configure this in the <a href=\"https://vercel.com/docs/vercel-sandbox/cli-reference#sandbox-snapshot\">CLI</a>.</p><div></div><p>Read the <a href=\"https://vercel.com/docs/vercel-sandbox/concepts/snapshots\">documentation</a> to learn more about snapshots.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/vercel-sandbox-snapshots-now-allow-custom-retention-periods\">Read more</a> </p>",
      "published_at": "2026-02-17T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Claude Sonnet 4.6 is live on AI Gateway",
      "url": "https://vercel.com/changelog/claude-sonnet-4-6-is-live-on-ai-gateway",
      "summary": "<p>Claude Sonnet 4.6 from Anthropic is now available on AI Gateway with the 1M token context window.</p><p>Sonnet 4.6 approaches Opus-level intelligence with strong improvements in agentic coding, code review, frontend UI quality, and computer use accuracy. The model proactively executes tasks, delegates to subagents, and parallelizes tool calls, with MCP support for scaled tool use. As a hybrid reasoning model, Sonnet 4.6 delivers both near-instant responses and extended thinking within the same model.</p><p>To use this model, set model to <code>anthropic/claude-sonnet-4.6</code> in the AI SDK. This model supports <code>effort</code> and thinking type <code>adaptive</code>:</p><div></div><p>AI Gateway provides a unified API for calling models, tracking usage and cost, and configuring retries, failover, and performance optimizations for higher-than-provider uptime. It includes built-in <a href=\"https://vercel.com/docs/observability/ai-sdk-observability\">observability</a>, <a href=\"https://vercel.com/docs/ai-gateway#bring-your-own-key\">Bring Your Own Key</a> support, and intelligent provider routing with automatic retries.</p><p>Learn more about <a href=\"https://vercel.com/ai-gateway\">AI Gateway</a>, view the <a href=\"https://vercel.com/ai-gateway/leaderboards\">AI Gateway model leaderboard</a> or try it in our <a href=\"https://vercel.com/ai-gateway/models/claude-sonnet-4.6\">model playground</a>.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/claude-sonnet-4-6-is-live-on-ai-gateway\">Read more</a> </p>",
      "published_at": "2026-02-17T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Improved streaming runtime logs exports",
      "url": "https://vercel.com/changelog/improved-streaming-runtime-logs-exports",
      "summary": "<p>With runtime logs, you can view and export your logs. Exports now stream directly to the browser - your download starts immediately and you can continue to use the Vercel dashboard while the export runs in the background. This eliminates the need to wait for large files to buffer.</p><p>Additionally, we've added two new options: You can now export exactly what's on your screen or all requests matching your current search.</p><div></div><p>All plans can export up to 10,000 requests per export, and <a href=\"https://vercel.com/docs/observability/observability-plus\">Observability Plus</a> subscribers can export up to 100,000 requests.</p><p>Exported log data is now indexed by request to ensure consistency with the <a href=\"https://vercel.com/docs/observability/runtime-logs\">Runtime Logs</a> dashboard interface. Export limits are now applied by request to ensure that the exported data matches the filtered requests shown on the dashboard.</p><p><a href=\"https://vercel.com/docs/logs/runtime\">Learn more about runtime logs</a>.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/improved-streaming-runtime-logs-exports\">Read more</a> </p>",
      "published_at": "2026-02-17T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "title": "Qwen 3.5 Plus is on AI Gateway",
      "url": "https://vercel.com/changelog/qwen-3-5-plus-is-on-ai-gateway",
      "summary": "<p>Qwen 3.5 Plus is now available on AI Gateway.</p><p>The model comes with a 1M context window and built-in adaptive tool use. Qwen 3.5 Plus excels at agentic workflows, thinking, searching, and using tools across multimodal contexts, making it well-suited for web development, frontend tasks, and turning instructions into working code. Compared to Qwen 3 VL, it delivers stronger performance in scientific problem solving and visual reasoning tasks.</p><p>To use this model, set model to <code>alibaba/qwen3.5-plus</code> in the AI SDK:</p><div></div><p>AI Gateway provides a unified API for calling models, tracking usage and cost, and configuring retries, failover, and performance optimizations for higher-than-provider uptime. It includes built-in <a href=\"https://vercel.com/docs/observability/ai-sdk-observability\">observability</a>, <a href=\"https://vercel.com/docs/ai-gateway#bring-your-own-key\">Bring Your Own Key</a> support, and intelligent provider routing with automatic retries.</p><p>Learn more about <a href=\"https://vercel.com/ai-gateway\">AI Gateway</a>, view the <a href=\"https://vercel.com/ai-gateway/leaderboards\">AI Gateway model leaderboard</a> or try it in our <a href=\"https://vercel.com/ai-gateway/models/qwen3.5-plus\">model playground</a>.</p> <p class=\"more\"> <a href=\"https://vercel.com/changelog/qwen-3-5-plus-is-on-ai-gateway\">Read more</a> </p>",
      "published_at": "2026-02-16T13:00:00.000Z",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "Teaching AI to read a map",
      "url": "https://research.google/blog/teaching-ai-to-read-a-map/",
      "summary": "Machine Perception",
      "published_at": "Tue, 17 Feb 2026 21:37:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "Scheduling in a changing world: Maximizing throughput with time-varying capacity",
      "url": "https://research.google/blog/scheduling-in-a-changing-world-maximizing-throughput-with-time-varying-capacity/",
      "summary": "Algorithms & Theory",
      "published_at": "Wed, 11 Feb 2026 10:34:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "Beyond one-on-one: Authoring, simulating, and testing dynamic human-AI group conversations",
      "url": "https://research.google/blog/beyond-one-on-one-authoring-simulating-and-testing-dynamic-human-ai-group-conversations/",
      "summary": "Human-Computer Interaction and Visualization",
      "published_at": "Tue, 10 Feb 2026 18:30:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "How AI trained on birds is surfacing underwater mysteries",
      "url": "https://research.google/blog/how-ai-trained-on-birds-is-surfacing-underwater-mysteries/",
      "summary": "Climate & Sustainability",
      "published_at": "Mon, 09 Feb 2026 18:38:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "How AI tools can redefine universal design to increase accessibility",
      "url": "https://research.google/blog/how-ai-agents-can-redefine-universal-design-to-increase-accessibility/",
      "summary": "Education Innovation",
      "published_at": "Thu, 05 Feb 2026 08:28:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "​Sequential Attention: Making AI models leaner and faster without sacrificing accuracy",
      "url": "https://research.google/blog/sequential-attention-making-ai-models-leaner-and-faster-without-sacrificing-accuracy/",
      "summary": "Algorithms & Theory",
      "published_at": "Wed, 04 Feb 2026 15:14:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "Collaborating on a nationwide randomized study of AI in real-world virtual care",
      "url": "https://research.google/blog/collaborating-on-a-nationwide-randomized-study-of-ai-in-real-world-virtual-care/",
      "summary": "Generative AI",
      "published_at": "Tue, 03 Feb 2026 18:15:01 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "Towards a science of scaling agent systems: When and why agent systems work",
      "url": "https://research.google/blog/towards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work/",
      "summary": "Generative AI",
      "published_at": "Wed, 28 Jan 2026 11:00:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "ATLAS: Practical scaling laws for multilingual models",
      "url": "https://research.google/blog/atlas-practical-scaling-laws-for-multilingual-models/",
      "summary": "Generative AI",
      "published_at": "Tue, 27 Jan 2026 18:58:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "Introducing GIST: The next stage in smart sampling",
      "url": "https://research.google/blog/introducing-gist-the-next-stage-in-smart-sampling/",
      "summary": "Algorithms & Theory",
      "published_at": "Fri, 23 Jan 2026 17:46:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "Small models, big results: Achieving superior intent extraction through decomposition",
      "url": "https://research.google/blog/small-models-big-results-achieving-superior-intent-extraction-through-decomposition/",
      "summary": "Generative AI",
      "published_at": "Thu, 22 Jan 2026 16:56:44 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "Unlocking health insights: Estimating advanced walking metrics with smartwatches",
      "url": "https://research.google/blog/unlocking-health-insights-estimating-advanced-walking-metrics-with-smartwatches/",
      "summary": "Health & Bioscience",
      "published_at": "Thu, 15 Jan 2026 22:56:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "Hard-braking events as indicators of road segment crash risk",
      "url": "https://research.google/blog/hard-braking-events-as-indicators-of-road-segment-crash-risk/",
      "summary": "Algorithms & Theory",
      "published_at": "Tue, 13 Jan 2026 22:44:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "Next generation medical image interpretation with MedGemma 1.5 and medical speech to text with MedASR",
      "url": "https://research.google/blog/next-generation-medical-image-interpretation-with-medgemma-15-and-medical-speech-to-text-with-medasr/",
      "summary": "Generative AI",
      "published_at": "Tue, 13 Jan 2026 20:57:16 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "Dynamic surface codes open new avenues for quantum error correction",
      "url": "https://research.google/blog/dynamic-surface-codes-open-new-avenues-for-quantum-error-correction/",
      "summary": "Quantum",
      "published_at": "Tue, 13 Jan 2026 17:32:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "NeuralGCM harnesses AI to better simulate long-range global precipitation",
      "url": "https://research.google/blog/neuralgcm-harnesses-ai-to-better-simulate-long-range-global-precipitation/",
      "summary": "Climate & Sustainability",
      "published_at": "Mon, 12 Jan 2026 17:52:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "Google Research 2025: Bolder breakthroughs, bigger impact",
      "url": "https://research.google/blog/google-research-2025-bolder-breakthroughs-bigger-impact/",
      "summary": "Year in Review",
      "published_at": "Thu, 18 Dec 2025 23:29:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "Gemini provides automated feedback for theoretical computer scientists at STOC 2026",
      "url": "https://research.google/blog/gemini-provides-automated-feedback-for-theoretical-computer-scientists-at-stoc-2026/",
      "summary": "Algorithms & Theory",
      "published_at": "Mon, 15 Dec 2025 17:37:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "Spotlight on innovation: Google-sponsored Data Science for Health Ideathon across Africa",
      "url": "https://research.google/blog/spotlight-on-innovation-google-sponsored-data-science-for-health-ideathon-across-africa/",
      "summary": "Conferences & Events",
      "published_at": "Fri, 12 Dec 2025 10:42:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "title": "A differentially private framework for gaining insights into AI chatbot use",
      "url": "https://research.google/blog/a-differentially-private-framework-for-gaining-insights-into-ai-chatbot-use/",
      "summary": "Generative AI",
      "published_at": "Wed, 10 Dec 2025 21:59:41 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Gemini 3.1 Pro: A smarter model for your most complex tasks",
      "url": "https://deepmind.google/blog/gemini-3-1-pro-a-smarter-model-for-your-most-complex-tasks/",
      "summary": "3.1 Pro is designed for tasks where a simple answer isn’t enough.",
      "published_at": "Thu, 19 Feb 2026 16:06:14 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "A new way to express yourself: Gemini can now create music",
      "url": "https://deepmind.google/blog/a-new-way-to-express-yourself-gemini-can-now-create-music/",
      "summary": "The Gemini app now features our most advanced music generation model Lyria 3, empowering anyone to make 30-second tracks using text or images.",
      "published_at": "Wed, 18 Feb 2026 16:01:38 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Accelerating discovery in India through AI-powered science and education",
      "url": "https://deepmind.google/blog/accelerating-discovery-in-india-through-ai-powered-science-and-education/",
      "summary": "Google DeepMind brings National Partnerships for AI initiative to India, scaling AI for science and education",
      "published_at": "Tue, 17 Feb 2026 13:42:20 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Gemini 3 Deep Think: Advancing science, research and engineering",
      "url": "https://deepmind.google/blog/gemini-3-deep-think-advancing-science-research-and-engineering/",
      "summary": "Our most specialized reasoning mode is now updated to solve modern science, research and engineering challenges.",
      "published_at": "Thu, 12 Feb 2026 16:15:09 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Accelerating Mathematical and Scientific Discovery with Gemini Deep Think",
      "url": "https://deepmind.google/blog/accelerating-mathematical-and-scientific-discovery-with-gemini-deep-think/",
      "summary": "Research papers point to the growing impact of Deep Think across fields",
      "published_at": "Mon, 09 Feb 2026 16:12:06 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Project Genie: Experimenting with infinite, interactive worlds",
      "url": "https://deepmind.google/blog/project-genie-experimenting-with-infinite-interactive-worlds/",
      "summary": "Google AI Ultra subscribers in the U.S. can try out Project Genie, an experimental research prototype that lets you create and explore worlds.",
      "published_at": "Thu, 29 Jan 2026 17:01:05 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "D4RT: Teaching AI to see the world in four dimensions",
      "url": "https://deepmind.google/blog/d4rt-teaching-ai-to-see-the-world-in-four-dimensions/",
      "summary": "D4RT: Unified, efficient 4D reconstruction and tracking up to 300x faster than prior methods.",
      "published_at": "Fri, 16 Jan 2026 10:39:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Veo 3.1 Ingredients to Video: More consistency, creativity and control",
      "url": "https://deepmind.google/blog/veo-3-1-ingredients-to-video-more-consistency-creativity-and-control/",
      "summary": "Our latest Veo update generates lively, dynamic clips that feel natural and engaging — and supports vertical video generation.",
      "published_at": "Tue, 13 Jan 2026 17:00:18 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Google's year in review: 8 areas with research breakthroughs in 2025",
      "url": "https://deepmind.google/blog/googles-year-in-review-8-areas-with-research-breakthroughs-in-2025/",
      "summary": "Google 2025 recap: Research breakthroughs of the year",
      "published_at": "Tue, 23 Dec 2025 17:01:02 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Gemini 3 Flash: frontier intelligence built for speed",
      "url": "https://deepmind.google/blog/gemini-3-flash-frontier-intelligence-built-for-speed/",
      "summary": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost.",
      "published_at": "Wed, 17 Dec 2025 11:58:17 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Gemma Scope 2: helping the AI safety community deepen understanding of complex language model behavior",
      "url": "https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/",
      "summary": "Open interpretability tools for language models are now available across the entire Gemma 3 family with the release of Gemma Scope 2.",
      "published_at": "Tue, 16 Dec 2025 10:14:24 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Improved Gemini audio models for powerful voice experiences",
      "url": "https://deepmind.google/blog/improved-gemini-audio-models-for-powerful-voice-experiences/",
      "summary": "",
      "published_at": "Fri, 12 Dec 2025 17:50:50 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Deepening our partnership with the UK AI Security Institute",
      "url": "https://deepmind.google/blog/deepening-our-partnership-with-the-uk-ai-security-institute/",
      "summary": "Google DeepMind and UK AI Security Institute (AISI) strengthen collaboration on critical AI safety and security research",
      "published_at": "Thu, 11 Dec 2025 00:06:40 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Strengthening our partnership with the UK government to support prosperity and security in the AI era",
      "url": "https://deepmind.google/blog/strengthening-our-partnership-with-the-uk-government-to-support-prosperity-and-security-in-the-ai-era/",
      "summary": "Deepening our partnership with the UK government to support prosperity and security in the AI era",
      "published_at": "Wed, 10 Dec 2025 14:59:21 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "FACTS Benchmark Suite: Systematically evaluating the factuality of large language models",
      "url": "https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/",
      "summary": "Systematically evaluating the factuality of large language models with the FACTS Benchmark Suite.",
      "published_at": "Tue, 09 Dec 2025 11:29:03 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Engineering more resilient crops for a warming climate",
      "url": "https://deepmind.google/blog/engineering-more-resilient-crops-for-a-warming-climate/",
      "summary": "Scientists are using AlphaFold to strengthen a photosynthesis enzyme for resilient, heat-tolerant crops.",
      "published_at": "Thu, 04 Dec 2025 16:23:24 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "AlphaFold: Five years of impact",
      "url": "https://deepmind.google/blog/alphafold-five-years-of-impact/",
      "summary": "Explore how AlphaFold has accelerated science and fueled a global wave of biological discovery.",
      "published_at": "Tue, 25 Nov 2025 16:00:12 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Revealing a key protein behind heart disease",
      "url": "https://deepmind.google/blog/revealing-a-key-protein-behind-heart-disease/",
      "summary": "AlphaFold has revealed the structure of a key protein behind heart disease",
      "published_at": "Tue, 25 Nov 2025 15:52:51 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "Google DeepMind supports U.S. Department of Energy on Genesis: a national mission to accelerate innovation and scientific discovery",
      "url": "https://deepmind.google/blog/google-deepmind-supports-us-department-of-energy-on-genesis/",
      "summary": "Google DeepMind and the DOE partner on Genesis, a new effort to accelerate science with AI.",
      "published_at": "Mon, 24 Nov 2025 14:12:03 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "title": "How we’re bringing AI image verification to the Gemini app",
      "url": "https://deepmind.google/blog/how-were-bringing-ai-image-verification-to-the-gemini-app/",
      "summary": "",
      "published_at": "Thu, 20 Nov 2025 15:13:19 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "meta-ai-blog",
      "source_name": "Meta AI Blog",
      "title": "Blog Posts",
      "url": "https://ai.meta.com/blog/forest-research-dino/",
      "summary": "",
      "published_at": "2026-02-09T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "meta-ai-blog",
      "source_name": "Meta AI Blog",
      "title": "Blog Posts",
      "url": "https://ai.meta.com/blog/upenn-dino-sam-helping-medical-triage/",
      "summary": "",
      "published_at": "2025-12-18T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "meta-ai-blog",
      "source_name": "Meta AI Blog",
      "title": "Blog Posts",
      "url": "https://ai.meta.com/blog/usra-sam-flood-emergencies/",
      "summary": "",
      "published_at": "2025-12-18T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "meta-ai-blog",
      "source_name": "Meta AI Blog",
      "title": "FEATURED",
      "url": "https://ai.meta.com/blog/sam-audio/",
      "summary": "",
      "published_at": "2025-12-16T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "meta-ai-blog",
      "source_name": "Meta AI Blog",
      "title": "Blog Posts",
      "url": "https://ai.meta.com/blog/segment-anything-conservation-x-wildlife-monitoring/",
      "summary": "",
      "published_at": "2025-11-24T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "meta-ai-blog",
      "source_name": "Meta AI Blog",
      "title": "Open Source",
      "url": "https://ai.meta.com/blog/executorch-reality-labs-on-device-ai/",
      "summary": "",
      "published_at": "2025-11-21T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "meta-ai-blog",
      "source_name": "Meta AI Blog",
      "title": "Computer Vision",
      "url": "https://ai.meta.com/blog/sam-3d/",
      "summary": "",
      "published_at": "2025-11-19T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "meta-ai-blog",
      "source_name": "Meta AI Blog",
      "title": "Computer Vision",
      "url": "https://ai.meta.com/blog/segment-anything-model-3/",
      "summary": "",
      "published_at": "2025-11-19T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "meta-ai-blog",
      "source_name": "Meta AI Blog",
      "title": "Open Source",
      "url": "https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/",
      "summary": "",
      "published_at": "2025-11-10T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "meta-ai-blog",
      "source_name": "Meta AI Blog",
      "title": "Agents Rule of Two: A Practical Approach to AI Agent Security",
      "url": "https://ai.meta.com/blog/practical-ai-agent-security/",
      "summary": "",
      "published_at": "2025-10-31T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "meta-ai-blog",
      "source_name": "Meta AI Blog",
      "title": "Open Source",
      "url": "https://ai.meta.com/blog/orakl-oncology-dinov2-accelerating-cancer-treatment/",
      "summary": "",
      "published_at": "2025-02-20T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "meta-ai-blog",
      "source_name": "Meta AI Blog",
      "title": "Developer Tools",
      "url": "https://ai.meta.com/blog/open-sourcing-ax-and-botorch-new-ai-tools-for-adaptive-experimentation/",
      "summary": "",
      "published_at": "2019-05-01T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "meta-ai-blog",
      "source_name": "Meta AI Blog",
      "title": "Blog Posts",
      "url": "https://ai.meta.com/blog/",
      "summary": "",
      "published_at": "",
      "source_via": "html"
    },
    {
      "source_id": "microsoft-foundry-blog",
      "source_name": "Microsoft Foundry Blog",
      "title": "Microsoft Agent Framework Reaches Release Candidate",
      "url": "https://devblogs.microsoft.com/foundry/microsoft-agent-framework-reaches-release-candidate/",
      "summary": "<p>We&#8217;re happy to announce that Microsoft Agent Framework is now in Release Candidate status for both .NET and Python. Release Candidate is an important milestone on the road to General Availability — it means the API surface is stable, and all features that we intend to release with version 1.0 are complete. Whether you&#8217;re building [&#8230;]</p> <p>The post <a href=\"https://devblogs.microsoft.com/foundry/microsoft-agent-framework-reaches-release-candidate/\">Microsoft Agent Framework Reaches Release Candidate</a> appeared first on <a href=\"https://devblogs.microsoft.com/foundry\">Microsoft Foundry Blog</a>.</p>",
      "published_at": "Fri, 20 Feb 2026 05:51:29 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "microsoft-foundry-blog",
      "source_name": "Microsoft Foundry Blog",
      "title": "What’s new in Microsoft Foundry | Dec 2025 & Jan 2026",
      "url": "https://devblogs.microsoft.com/foundry/whats-new-in-microsoft-foundry-dec-2025-jan-2026/",
      "summary": "<p>Catch up on all the new models, audio updates, fine-tuning expansions, and SDK updates from Microsoft Foundry spanning December 2025 and January 2026 — including GPT-5.2, Codex Max, DeepSeek V3.2, FLUX.2, and the azure-ai-projects v2 beta consolidation.</p> <p>The post <a href=\"https://devblogs.microsoft.com/foundry/whats-new-in-microsoft-foundry-dec-2025-jan-2026/\">What&#8217;s new in Microsoft Foundry | Dec 2025 &#038; Jan 2026</a> appeared first on <a href=\"https://devblogs.microsoft.com/foundry\">Microsoft Foundry Blog</a>.</p>",
      "published_at": "Thu, 19 Feb 2026 01:01:09 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "microsoft-foundry-blog",
      "source_name": "Microsoft Foundry Blog",
      "title": "DPO Fine-Tuning Using Microsoft Foundry SDK",
      "url": "https://devblogs.microsoft.com/foundry/dpo-fine-tuning-using-microsoft-foundry-sdk/",
      "summary": "<p>In the rapidly evolving landscape of large language models (LLMs), achieving precise control over model behavior while maintaining quality has become a critical challenge. While models like GPT-4 demonstrate impressive capabilities, ensuring their outputs align with human preferences—whether for safety, helpfulness, or style—requires sophisticated fine-tuning techniques. Direct Preference Optimization (DPO) represents a breakthrough approach that [&#8230;]</p> <p>The post <a href=\"https://devblogs.microsoft.com/foundry/dpo-fine-tuning-using-microsoft-foundry-sdk/\">DPO Fine-Tuning Using Microsoft Foundry SDK</a> appeared first on <a href=\"https://devblogs.microsoft.com/foundry\">Microsoft Foundry Blog</a>.</p>",
      "published_at": "Fri, 13 Feb 2026 23:13:44 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "microsoft-foundry-blog",
      "source_name": "Microsoft Foundry Blog",
      "title": "Beyond the Prompt – Why and How to Fine-tune Your Own Models",
      "url": "https://devblogs.microsoft.com/foundry/beyond-the-prompt-why-and-how-to-fine-tune-your-own-models/",
      "summary": "<p>Large Language Models (LLMs) have reached a point where general intelligence is no longer the bottleneck. The real challenge in enterprise AI systems behavioral alignment ensuring models that produce consistent, reliable, policy-compliant outputs on a scale. Prompt engineering and Retrieval-Augmented Generation (RAG) are powerful but they do not change model behavior. Fine-tuning will solve this [&#8230;]</p> <p>The post <a href=\"https://devblogs.microsoft.com/foundry/beyond-the-prompt-why-and-how-to-fine-tune-your-own-models/\">Beyond the Prompt &#8211; Why and How to Fine-tune Your Own Models</a> appeared first on <a href=\"https://devblogs.microsoft.com/foundry\">Microsoft Foundry Blog</a>.</p>",
      "published_at": "Wed, 11 Feb 2026 17:29:15 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "microsoft-foundry-blog",
      "source_name": "Microsoft Foundry Blog",
      "title": "Building an AI Skills Executor in .NET: Bringing Anthropic’s Agent Pattern to the Microsoft Ecosystem",
      "url": "https://devblogs.microsoft.com/foundry/dotnet-ai-skills-executor-azure-openai-mcp/",
      "summary": "<p>We welcome guest author, Matt Kruczek, to share how to build a complete Skills Executor in .NET, combining Azure OpenAI with the Model Context Protocol (MCP) C# SDK to bring the same agentic patterns to the Microsoft ecosystem. You’ll see how to define reusable skills, connect local and external tools, and orchestrate intelligent agent loops—so enterprise teams can standardize AI behavior without hardcoding business logic.</p> <p>The post <a href=\"https://devblogs.microsoft.com/foundry/dotnet-ai-skills-executor-azure-openai-mcp/\">Building an AI Skills Executor in .NET: Bringing Anthropic&#8217;s Agent Pattern to the Microsoft Ecosystem</a> appeared first on <a href=\"https://devblogs.microsoft.com/foundry\">Microsoft Foundry Blog</a>.</p>",
      "published_at": "Fri, 06 Feb 2026 19:11:04 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "microsoft-foundry-blog",
      "source_name": "Microsoft Foundry Blog",
      "title": "What’s new in Microsoft Foundry | October and November 2025",
      "url": "https://devblogs.microsoft.com/foundry/whats-new-in-microsoft-foundry-oct-nov-2025/",
      "summary": "<p>Azure AI Foundry is now Microsoft Foundry. Read the latest announcements about agents, models, tools and more.</p> <p>The post <a href=\"https://devblogs.microsoft.com/foundry/whats-new-in-microsoft-foundry-oct-nov-2025/\">What’s new in Microsoft Foundry | October and November 2025</a> appeared first on <a href=\"https://devblogs.microsoft.com/foundry\">Microsoft Foundry Blog</a>.</p>",
      "published_at": "Fri, 19 Dec 2025 04:55:08 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "microsoft-foundry-blog",
      "source_name": "Microsoft Foundry Blog",
      "title": "Foundry IQ in Microsoft Agent Framework",
      "url": "https://devblogs.microsoft.com/foundry/foundry-iq-agent-framework-integration/",
      "summary": "<p>Build enterprise-grade RAG agents with Foundry IQ Knowledge Bases in ~20 lines of Python. Learn how the Azure AI Search Context Provider brings intelligent, multi-hop retrieval to the Microsoft Agent Framework—no fragmented pipelines, just plug in the knowledge your agent needs.</p> <p>The post <a href=\"https://devblogs.microsoft.com/foundry/foundry-iq-agent-framework-integration/\">Foundry IQ in Microsoft Agent Framework</a> appeared first on <a href=\"https://devblogs.microsoft.com/foundry\">Microsoft Foundry Blog</a>.</p>",
      "published_at": "Sun, 07 Dec 2025 10:55:00 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "microsoft-foundry-blog",
      "source_name": "Microsoft Foundry Blog",
      "title": "Announcing Foundry MCP Server (preview) in the cloud, speeding up AI development with Microsoft Foundry",
      "url": "https://devblogs.microsoft.com/foundry/announcing-foundry-mcp-server-preview-speeding-up-ai-dev-with-microsoft-foundry/",
      "summary": "<p>MCP (Model Context Protocol) is a standard protocol that enables AI agents to securely connect with apps, data, and systems, supporting easy interoperability and seamless platform expansion. At Ignite, Microsoft Foundry introduced Foundry Tools, which serves as a central hub for discovering, connecting, and managing both public and private MCP tools securely, simplifying integration across [&#8230;]</p> <p>The post <a href=\"https://devblogs.microsoft.com/foundry/announcing-foundry-mcp-server-preview-speeding-up-ai-dev-with-microsoft-foundry/\">Announcing Foundry MCP Server (preview) in the cloud, speeding up AI development with Microsoft Foundry</a> appeared first on <a href=\"https://devblogs.microsoft.com/foundry\">Microsoft Foundry Blog</a>.</p>",
      "published_at": "Wed, 03 Dec 2025 19:15:11 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "microsoft-foundry-blog",
      "source_name": "Microsoft Foundry Blog",
      "title": "⭐Upcoming Virtual Event⭐ AI Dev Days, Level-Up Your AI Skills with Microsoft Reactor",
      "url": "https://devblogs.microsoft.com/foundry/ai-dev-days-december-2025/",
      "summary": "<p>Join us for AI Dev Days, a two-day virtual event exploring the latest Microsoft Azure, Foundry and GitHub innovations. Whether you’re modernizing legacy apps, building with agents, or exploring the newest AI models, this is your moment to skill up, ship faster, and connect with experts.</p> <p>The post <a href=\"https://devblogs.microsoft.com/foundry/ai-dev-days-december-2025/\">⭐Upcoming Virtual Event⭐ AI Dev Days, Level-Up Your AI Skills with Microsoft Reactor</a> appeared first on <a href=\"https://devblogs.microsoft.com/foundry\">Microsoft Foundry Blog</a>.</p>",
      "published_at": "Wed, 03 Dec 2025 03:15:13 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "microsoft-foundry-blog",
      "source_name": "Microsoft Foundry Blog",
      "title": "Introducing Memory in Foundry Agent Service",
      "url": "https://devblogs.microsoft.com/foundry/introducing-memory-in-foundry-agent-service/",
      "summary": "<p>Give your agents the power to remember Imagine your agent never asks the same question twice. Until now, most agents have been stateless. Each conversation resets to zero, forgetting what users said just minutes ago or weeks ago. Developers tried to bridge this gap with homegrown solutions — storing embeddings in databases, manually retrieving prior [&#8230;]</p> <p>The post <a href=\"https://devblogs.microsoft.com/foundry/introducing-memory-in-foundry-agent-service/\">Introducing Memory in Foundry Agent Service</a> appeared first on <a href=\"https://devblogs.microsoft.com/foundry\">Microsoft Foundry Blog</a>.</p>",
      "published_at": "Tue, 25 Nov 2025 16:00:48 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Amazon SageMaker AI in 2025, a year in review part 1: Flexible Training Plans and improvements to price performance for inference workloads",
      "url": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-ai-in-2025-a-year-in-review-part-1-flexible-training-plans-and-improvements-to-price-performance-for-inference-workloads/",
      "summary": "In 2025, Amazon SageMaker AI saw dramatic improvements to core infrastructure offerings along four dimensions: capacity, price performance, observability, and usability. In this series of posts, we discuss these various improvements and their benefits. In Part 1, we discuss capacity improvements with the launch of Flexible Training Plans. We also describe improvements to price performance for inference workloads. In Part 2, we discuss enhancements made to observability, model customization, and model hosting.",
      "published_at": "Fri, 20 Feb 2026 20:26:47 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Amazon SageMaker AI in 2025, a year in review part 2: Improved observability and enhanced features for SageMaker AI model customization and hosting",
      "url": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-ai-in-2025-a-year-in-review-part-2-improved-observability-and-enhanced-features-for-sagemaker-ai-model-customization-and-hosting/",
      "summary": "In 2025, Amazon SageMaker AI made several improvements designed to help you train, tune, and host generative AI workloads. In Part 1 of this series, we discussed Flexible Training Plans and price performance improvements made to inference components. In this post, we discuss enhancements made to observability, model customization, and model hosting. These improvements facilitate a whole new class of customer use cases to be hosted on SageMaker AI.",
      "published_at": "Fri, 20 Feb 2026 20:26:30 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Integrate external tools with Amazon Quick Agents using Model Context Protocol (MCP)",
      "url": "https://aws.amazon.com/blogs/machine-learning/integrate-external-tools-with-amazon-quick-agents-using-model-context-protocol-mcp/",
      "summary": "In this post, you’ll use a six-step checklist to build a new MCP server or validate and adjust an existing MCP server for Amazon&nbsp;Quick integration.&nbsp;The Amazon&nbsp;Quick User Guide describes the MCP client behavior and constraints. This is a “How to” guide for detailed implementation required by 3P partners to integrate with Amazon Quick with MCP.",
      "published_at": "Fri, 20 Feb 2026 16:26:21 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Build AI workflows on Amazon EKS with Union.ai and Flyte",
      "url": "https://aws.amazon.com/blogs/machine-learning/build-ai-workflows-on-amazon-eks-with-union-ai-and-flyte/",
      "summary": "In this post, we explain how you can use the Flyte Python SDK to orchestrate and scale AI/ML workflows. We explore how the Union.ai 2.0 system enables deployment of Flyte on Amazon Elastic Kubernetes Service (Amazon EKS), integrating seamlessly with AWS services like Amazon Simple Storage Service (Amazon S3), Amazon Aurora, AWS Identity and Access Management (IAM), and Amazon CloudWatch. We explore the solution through an AI workflow example, using the new Amazon S3 Vectors service.",
      "published_at": "Thu, 19 Feb 2026 16:28:21 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Amazon Quick now supports key pair authentication to Snowflake data source",
      "url": "https://aws.amazon.com/blogs/machine-learning/amazon-quick-suite-now-supports-key-pair-authentication-to-snowflake-data-source/",
      "summary": "In this blog post, we will guide you through establishing data source connectivity between Amazon Quick Sight and Snowflake through secure key pair authentication.",
      "published_at": "Thu, 19 Feb 2026 16:06:41 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Build unified intelligence with Amazon Bedrock AgentCore",
      "url": "https://aws.amazon.com/blogs/machine-learning/build-unified-intelligence-with-amazon-bedrock-agentcore/",
      "summary": "In this post, we demonstrate how to build unified intelligence systems using Amazon Bedrock AgentCore through our real-world implementation of the Customer Agent and Knowledge Engine (CAKE).",
      "published_at": "Wed, 18 Feb 2026 23:54:29 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Evaluating AI agents: Real-world lessons from building agentic systems at Amazon",
      "url": "https://aws.amazon.com/blogs/machine-learning/evaluating-ai-agents-real-world-lessons-from-building-agentic-systems-at-amazon/",
      "summary": "In this post, we present a comprehensive evaluation framework for Amazon agentic AI systems that addresses the complexity of agentic AI applications at Amazon&nbsp;through two core components: a generic evaluation workflow that standardizes assessment procedures across diverse agent implementations, and an agent evaluation library that provides systematic measurements and metrics in Amazon Bedrock AgentCore Evaluations, along with&nbsp;Amazon use case-specific evaluation approaches and metrics.&nbsp;",
      "published_at": "Wed, 18 Feb 2026 19:21:28 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Customize AI agent browsing with proxies, profiles, and extensions in Amazon Bedrock AgentCore Browser",
      "url": "https://aws.amazon.com/blogs/machine-learning/customize-ai-agent-browsing-with-proxies-profiles-and-extensions-in-amazon-bedrock-agentcore-browser/",
      "summary": "Today, we are announcing three new capabilities that address these requirements: proxy configuration, browser profiles, and browser extensions. Together, these features give you fine-grained control over how your AI agents interact with the web. This post will walk through each capability with configuration examples and practical use cases to help you get started.",
      "published_at": "Fri, 13 Feb 2026 22:57:34 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "AI meets HR: Transforming talent acquisition with Amazon Bedrock",
      "url": "https://aws.amazon.com/blogs/machine-learning/ai-meets-hr-transforming-talent-acquisition-with-amazon-bedrock/",
      "summary": "In this post, we show how to create an AI-powered recruitment system using Amazon Bedrock, Amazon Bedrock Knowledge Bases, AWS Lambda, and other AWS services to enhance job description creation, candidate communication, and interview preparation while maintaining human oversight.",
      "published_at": "Thu, 12 Feb 2026 20:18:58 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Build long-running MCP servers on Amazon Bedrock AgentCore with Strands Agents integration",
      "url": "https://aws.amazon.com/blogs/machine-learning/build-long-running-mcp-servers-on-amazon-bedrock-agentcore-with-strands-agents-integration/",
      "summary": "In this post, we provide you with a comprehensive approach to achieve this. First, we introduce a context message strategy that maintains continuous communication between servers and clients during extended operations. Next, we develop an asynchronous task management framework that allows your AI agents to initiate long-running processes without blocking other operations. Finally, we demonstrate how to bring these strategies together with Amazon Bedrock AgentCore and Strands Agents to build production-ready AI agents that can handle complex, time-intensive operations reliably.",
      "published_at": "Thu, 12 Feb 2026 20:16:20 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "NVIDIA Nemotron 3 Nano 30B MoE model is now available in Amazon SageMaker JumpStart",
      "url": "https://aws.amazon.com/blogs/machine-learning/nvidia-nemotron-3-nano-30b-is-now-available-in-amazon-sagemaker-jumpstart/",
      "summary": "Today we’re excited to announce that the NVIDIA Nemotron 3 Nano 30B model with &nbsp;3B active parameters is now generally available in the Amazon SageMaker JumpStart model catalog. You can accelerate innovation and deliver tangible business value with Nemotron 3 Nano on Amazon Web Services (AWS) without having to manage model deployment complexities. You can power your generative AI applications with Nemotron capabilities using the managed deployment capabilities offered by SageMaker JumpStart.",
      "published_at": "Wed, 11 Feb 2026 19:38:47 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Swann provides Generative AI to millions of IoT Devices using Amazon Bedrock",
      "url": "https://aws.amazon.com/blogs/machine-learning/swann-provides-generative-ai-to-millions-of-iot-devices-using-amazon-bedrock/",
      "summary": "This post shows you how to implement intelligent notification filtering using Amazon Bedrock and its gen-AI capabilities. You'll learn model selection strategies, cost optimization techniques, and architectural patterns for deploying gen-AI at IoT scale, based on Swann Communications deployment across millions of devices.",
      "published_at": "Wed, 11 Feb 2026 15:48:15 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "How LinqAlpha assesses investment theses using Devil’s Advocate on Amazon Bedrock",
      "url": "https://aws.amazon.com/blogs/machine-learning/how-linqalpha-assesses-investment-theses-using-devils-advocate-on-amazon-bedrock/",
      "summary": "LinqAlpha is a Boston-based multi-agent AI system built specifically for institutional investors. The system supports and streamlines agentic workflows across company screening, primer generation, stock price catalyst mapping, and now, pressure-testing investment ideas through a new AI agent called Devil’s Advocate. In this post, we share how LinqAlpha uses Amazon Bedrock to build and scale Devil’s Advocate.",
      "published_at": "Wed, 11 Feb 2026 15:45:30 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "How Amazon uses Amazon Nova models to automate operational readiness testing for new fulfillment centers",
      "url": "https://aws.amazon.com/blogs/machine-learning/how-amazon-uses-amazon-nova-models-to-automate-operational-readiness-testing-for-new-fulfillment-centers/",
      "summary": "In this post, we discuss how&nbsp;Amazon Nova&nbsp;in&nbsp;Amazon Bedrock&nbsp;can be used to implement an AI-powered image recognition solution that automates the detection and validation of module components, significantly reducing manual verification efforts and improving accuracy.",
      "published_at": "Tue, 10 Feb 2026 18:34:09 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Iberdrola enhances IT operations using Amazon Bedrock AgentCore",
      "url": "https://aws.amazon.com/blogs/machine-learning/iberdrola-enhances-it-operations-using-amazon-bedrock-agentcore/",
      "summary": "Iberdrola, one of the world’s largest utility companies, has embraced cutting-edge AI technology to revolutionize its IT operations in ServiceNow. Through its partnership with AWS, Iberdrola implemented different agentic architectures using Amazon Bedrock AgentCore, targeting three key areas: optimizing change request validation in the draft phase, enriching incident management with contextual intelligence, and simplifying change model selection using conversational AI. These innovations reduce bottlenecks, help teams accelerate ticket resolution, and deliver consistent and high-quality data handling throughout the organization.",
      "published_at": "Tue, 10 Feb 2026 18:31:57 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Building real-time voice assistants with Amazon Nova Sonic compared to cascading architectures",
      "url": "https://aws.amazon.com/blogs/machine-learning/building-real-time-voice-assistants-with-amazon-nova-sonic-compared-to-cascading-architectures/",
      "summary": "Amazon Nova Sonic&nbsp;delivers real-time, human-like voice conversations through the bidirectional streaming interface. In this post, you learn how Amazon Nova Sonic can solve some of the challenges faced by cascaded approaches, simplify building voice AI agents, and provide natural conversational capabilities. We also provide guidance on when to choose each approach to help you make informed decisions for your voice AI projects.",
      "published_at": "Tue, 10 Feb 2026 18:29:05 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Automated Reasoning checks rewriting chatbot reference implementation",
      "url": "https://aws.amazon.com/blogs/machine-learning/automated-reasoning-checks-rewriting-chatbot-reference-implementation/",
      "summary": "This blog post dives deeper into the implementation architecture for the Automated Reasoning checks rewriting chatbot.",
      "published_at": "Mon, 09 Feb 2026 19:34:05 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Scale LLM fine-tuning with Hugging Face and Amazon SageMaker AI",
      "url": "https://aws.amazon.com/blogs/machine-learning/scale-llm-fine-tuning-with-hugging-face-and-amazon-sagemaker-ai/",
      "summary": "In this post, we show how this integrated approach transforms enterprise LLM fine-tuning from a complex, resource-intensive challenge into a streamlined, scalable solution for achieving better model performance in domain-specific applications.",
      "published_at": "Mon, 09 Feb 2026 16:48:46 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "New Relic transforms productivity with generative AI on AWS",
      "url": "https://aws.amazon.com/blogs/machine-learning/new-relic-transforms-productivity-with-generative-ai-on-aws/",
      "summary": "Working with the Generative AI Innovation Center, New Relic NOVA (New Relic Omnipresence Virtual Assistant) evolved from a knowledge assistant into a comprehensive productivity engine. We explore the technical architecture, development journey, and key lessons learned in building an enterprise-grade AI solution that delivers measurable productivity gains at scale.",
      "published_at": "Mon, 09 Feb 2026 16:45:16 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "title": "Accelerate agentic application development with a full-stack starter template for Amazon Bedrock AgentCore",
      "url": "https://aws.amazon.com/blogs/machine-learning/accelerate-agentic-application-development-with-a-full-stack-starter-template-for-amazon-bedrock-agentcore/",
      "summary": "In this post, you will learn how to deploy Fullstack AgentCore Solution Template (FAST) to your Amazon Web Services (AWS) account, understand its architecture, and see how to extend it for your requirements. You will learn how to build your own agent while FAST handles authentication, infrastructure as code (IaC), deployment pipelines, and service integration.",
      "published_at": "Mon, 09 Feb 2026 16:40:58 +0000",
      "source_via": "rss"
    },
    {
      "source_id": "mistral-news",
      "source_name": "Mistral News",
      "title": "Voxtral transcribes at the speed of sound. Research February 4, 2026 Mistral AI Precision diarization, real-time transcription, and a new audio playground.",
      "url": "https://mistral.ai/news/voxtral-transcribe-2",
      "summary": "",
      "published_at": "2026-02-04T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "The Cost of Overthinking: Why Reasoning Models Fail at Document Parsing",
      "url": "https://www.llamaindex.ai/blog/the-cost-of-overthinking-why-reasoning-models-fail-at-document-parsing",
      "summary": "",
      "published_at": "2026-02-19T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "Beyond Full-Text Extraction: Why Page-Level Granularity Matters",
      "url": "https://www.llamaindex.ai/blog/beyond-full-text-extraction-why-page-level-granularity-matters",
      "summary": "",
      "published_at": "2026-02-17T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "Long Horizon Document Agents",
      "url": "https://www.llamaindex.ai/blog/long-horizon-document-agents",
      "summary": "",
      "published_at": "2026-02-12T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "Skills vs MCP tools for agents: when to use what",
      "url": "https://www.llamaindex.ai/blog/skills-vs-mcp-tools-for-agents-when-to-use-what",
      "summary": "",
      "published_at": "2026-02-03T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "LlamaAgents Builder: Idea To Deployed Agent in Minutes",
      "url": "https://www.llamaindex.ai/blog/llamaagents-builder-idea-to-deployed-agent-in-minutes",
      "summary": "",
      "published_at": "2026-01-28T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "LlamaIndex Newsletter 2026-01-27",
      "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2026-01-27",
      "summary": "",
      "published_at": "2026-01-27T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "Building Back Office Agents with LlamaParse & LlamaAgents",
      "url": "https://www.llamaindex.ai/blog/building-back-office-agents-with-llamacloud-and-llamaagents",
      "summary": "",
      "published_at": "2026-01-27T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "Announcing New LlamaParse SDKs and Parse API v2",
      "url": "https://www.llamaindex.ai/blog/announcing-new-llamacloud-sdks-and-parse-api-v2",
      "summary": "",
      "published_at": "2026-01-22T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "Files Are All You Need",
      "url": "https://www.llamaindex.ai/blog/files-are-all-you-need",
      "summary": "",
      "published_at": "2026-01-15T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "Did Filesystem Tools Kill Vector Search?",
      "url": "https://www.llamaindex.ai/blog/did-filesystem-tools-kill-vector-search",
      "summary": "",
      "published_at": "2026-01-13T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "LlamaIndex Newsletter 2025-12-16",
      "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2025-12-16",
      "summary": "",
      "published_at": "2025-12-16T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "Making Coding Agents Safe: Using LlamaIndex to Secure Filesystem Access",
      "url": "https://www.llamaindex.ai/blog/making-coding-agents-safe-using-llamaindex",
      "summary": "",
      "published_at": "2025-12-15T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "Announcing LlamaSplit Public Beta: Divide Long Document into Clear, Targeted Sections",
      "url": "https://www.llamaindex.ai/blog/split-document-into-clear-targeted-sections-with-llamasplit",
      "summary": "",
      "published_at": "2025-12-09T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "LlamaIndex Newsletter 2025-12-09",
      "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2025-12-09",
      "summary": "",
      "published_at": "2025-12-09T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "OlmOCR-Bench Review — Insights and Pitfalls on an OCR Benchmark",
      "url": "https://www.llamaindex.ai/blog/olmocr-bench-review-insights-and-pitfalls-on-an-ocr-benchmark",
      "summary": "",
      "published_at": "2025-12-04T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "LlamaIndex Newsletter 2025-12-02",
      "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2025-12-02",
      "summary": "",
      "published_at": "2025-12-02T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "AI Document Parsing: LLMs Are Redefining How Machines Read and Understand Documents",
      "url": "https://www.llamaindex.ai/blog/ai-document-parsing-llms-are-redefining-how-machines-read-and-understand-documents",
      "summary": "",
      "published_at": "2025-11-20T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "Announcing LlamaAgents Open Preview: Build, Serve & Deploy Document Agents",
      "url": "https://www.llamaindex.ai/blog/llamaagents-build-serve-and-deploy-document-agents",
      "summary": "",
      "published_at": "2025-11-18T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "LlamaIndex Newsletter 2025-11-18",
      "url": "https://www.llamaindex.ai/blog/llamaindex-newsletter-2025-11-18",
      "summary": "",
      "published_at": "2025-11-18T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "title": "Document AI: The Next Evolution of Intelligent Document Processing",
      "url": "https://www.llamaindex.ai/blog/document-ai-the-next-evolution-of-intelligent-document-processing",
      "summary": "",
      "published_at": "2025-11-14T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "FEATURED The state of agentic AI in 2026 February 11, 2026 Learn more",
      "url": "https://www.crewai.com/blog/the-state-of-agentic-ai-in-2026",
      "summary": "",
      "published_at": "2026-02-11T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Crew AI Enterprise How to build Agentic Systems: The Missing Architecture for Production AI Agents João (Joe) Moura December 15, 2025",
      "url": "https://www.crewai.com/blog/how-to-build-agentic-systems-the-missing-architecture-for-production-ai-agents",
      "summary": "",
      "published_at": "2025-12-15T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Crew AI Enterprise CrewAI OSS 1.0 - We are going GA João (Joe) Moura October 20, 2025",
      "url": "https://www.crewai.com/blog/crewai-oss-1-0---we-are-going-ga",
      "summary": "",
      "published_at": "2025-10-20T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Tutorials CrewAI AMP - The Agent Management Platform João (Joe) Moura October 2, 2025",
      "url": "https://www.crewai.com/blog/crewai-amp---the-agent-management-platform",
      "summary": "",
      "published_at": "2025-10-02T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Tutorials CrewAI on 2025 IA Enablers List with OpenAI and Anthropic João (Joe) Moura August 28, 2025",
      "url": "https://www.crewai.com/blog/crewai-on-2025-ia-enablers-list-with-openai-and-anthropic",
      "summary": "",
      "published_at": "2025-08-28T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Tutorials Running Crews on NVIDIA’s Newest Model —Here’s What Happened João (Joe) Moura August 11, 2025",
      "url": "https://www.crewai.com/blog/running-crews-on-nvidias-newest-model---heres-what-happened",
      "summary": "",
      "published_at": "2025-08-11T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Tutorials PwC Choses CrewAI to Help Power TheirGlobal Agent OS João (Joe) Moura July 30, 2025",
      "url": "https://www.crewai.com/blog/pwc-choses-crewai-to-help-power-theirglobal-agent-os",
      "summary": "",
      "published_at": "2025-07-30T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Tutorials Build Agents to be Dependable João (Joe) Moura July 1, 2025",
      "url": "https://www.crewai.com/blog/build-agents-to-be-dependable",
      "summary": "",
      "published_at": "2025-07-01T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Tutorials Creating a center of gravity for the Agentic AI ecosystem Shane Johnson May 22, 2025",
      "url": "https://www.crewai.com/blog/creating-a-center-of-gravity-for-the-agentic-ai-ecosystem",
      "summary": "",
      "published_at": "2025-05-22T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Tutorials Enabling domain experts to build and deploy agentic workflows without the need to write code Shane Johnson May 21, 2025",
      "url": "https://www.crewai.com/blog/enabling-domain-experts-to-build-and-deploy-agentic-workflows-without-the-need-to-write-code",
      "summary": "",
      "published_at": "2025-05-21T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Crew AI Enterprise How CrewAI is evolving beyond orchestration to create the most powerful Agentic AI platform Shane Johnson May 20, 2025",
      "url": "https://www.crewai.com/blog/how-crewai-is-evolving-beyond-orchestration-to-create-the-most-powerful-agentic-ai-platform",
      "summary": "",
      "published_at": "2025-05-20T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Crew AI Enterprise Unlocking agent-native transformation with CrewAI Factory and NVIDIA Shane Johnson May 19, 2025",
      "url": "https://www.crewai.com/blog/unlocking-agent-native-transformation-with-crewai-factory-and-nvidia",
      "summary": "",
      "published_at": "2025-05-19T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Crew AI Enterprise What Matters in AI Agents João (Joe) Moura May 13, 2025",
      "url": "https://www.crewai.com/blog/what-matters-in-ai-agents",
      "summary": "",
      "published_at": "2025-05-13T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Crew AI Enterprise Build your First CrewAI Agents João (Joe) Moura May 8, 2025",
      "url": "https://www.crewai.com/blog/build-your-first-crewai-agents",
      "summary": "",
      "published_at": "2025-05-08T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Crew AI Enterprise Enhancing CrewAI with CopilotKit Integration João (Joe) Moura March 27, 2025",
      "url": "https://www.crewai.com/blog/enhancing-crewai-with-copilotkit-integration",
      "summary": "",
      "published_at": "2025-03-27T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Crew AI Enterprise On Prem Agentic AI Infrastructure: HPE and CrewAI João (Joe) Moura March 26, 2025",
      "url": "https://www.crewai.com/blog/on-prem-agentic-ai-infrastructure-hpe-and-crewai",
      "summary": "",
      "published_at": "2025-03-26T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Crew AI Factory How CrewAI Delivers Enterprise-Level Complexity Without Compromise João (Joe) Moura March 19, 2025",
      "url": "https://www.crewai.com/blog/how-crewai-delivers-enterprise-level-complexity-without-compromise",
      "summary": "",
      "published_at": "2025-03-19T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Crew AI Enterprise CrewAI Integration With NVIDIA AI For Production-Grade AI Agents João (Joe) Moura January 7, 2025",
      "url": "https://www.crewai.com/blog/crewai-integration-with-nvidia-ai-for-production-grade-ai-agents",
      "summary": "",
      "published_at": "2025-01-07T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Crew AI Enterprise CrewAI + Cloudera: AI Agents with Precision in Enterprise Workflows João (Joe) Moura December 11, 2024",
      "url": "https://www.crewai.com/blog/crewai-cloudera-ai-agents-with-precision-in-enterprise-workflows",
      "summary": "",
      "published_at": "2024-12-11T00:00:00+00:00",
      "source_via": "html"
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "title": "Crew AI Enterprise How Waynabox is changing Travel Planning with CrewAI João (Joe) Moura November 20, 2024",
      "url": "https://www.crewai.com/blog/how-waynabox-is-changing-travel-planning-with-crewai",
      "summary": "",
      "published_at": "2024-11-20T00:00:00+00:00",
      "source_via": "html"
    }
  ],
  "sources": [
    {
      "source_id": "openai-news-engineering",
      "source_name": "OpenAI News (Engineering)",
      "homepage": "https://openai.com/news/engineering/",
      "mode": "rss_config",
      "feed_url": "https://openai.com/news/rss.xml",
      "feed_status": "confirmed",
      "check_method": "configured",
      "status": "ok",
      "item_count": 20,
      "checked_at": "2026-02-22T02:52:09.942459+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "openai-developers-blog",
      "source_name": "OpenAI Developers Blog",
      "homepage": "https://developers.openai.com/blog/",
      "mode": "rss_config",
      "feed_url": "https://developers.openai.com/rss.xml",
      "feed_status": "confirmed",
      "check_method": "configured",
      "status": "ok",
      "item_count": 6,
      "checked_at": "2026-02-22T02:52:10.633532+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "anthropic-engineering",
      "source_name": "Anthropic Engineering",
      "homepage": "https://www.anthropic.com/engineering",
      "mode": "html",
      "feed_url": null,
      "feed_status": "not_found",
      "check_method": "http_probe",
      "status": "ok",
      "item_count": 18,
      "checked_at": "2026-02-22T02:52:30.686864+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "claude-blog",
      "source_name": "Claude Blog",
      "homepage": "https://claude.com/blog",
      "mode": "html",
      "feed_url": null,
      "feed_status": "not_found",
      "check_method": "http_probe",
      "status": "ok",
      "item_count": 20,
      "checked_at": "2026-02-22T02:52:34.416612+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "manus-blog",
      "source_name": "Manus Blog",
      "homepage": "https://manus.im/blog",
      "mode": "html",
      "feed_url": null,
      "feed_status": "not_found",
      "check_method": "http_probe",
      "status": "ok",
      "item_count": 20,
      "checked_at": "2026-02-22T02:52:42.462026+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "langchain-blog",
      "source_name": "LangChain Blog",
      "homepage": "https://blog.langchain.com/",
      "mode": "rss_config",
      "feed_url": "https://blog.langchain.com/rss/",
      "feed_status": "confirmed",
      "check_method": "configured",
      "status": "ok",
      "item_count": 15,
      "checked_at": "2026-02-22T02:52:43.134907+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "huggingface-blog",
      "source_name": "Hugging Face Blog",
      "homepage": "https://huggingface.co/blog/",
      "mode": "rss_config",
      "feed_url": "https://huggingface.co/blog/feed.xml",
      "feed_status": "confirmed",
      "check_method": "configured",
      "status": "ok",
      "item_count": 20,
      "checked_at": "2026-02-22T02:52:44.194817+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "github-blog",
      "source_name": "GitHub Blog",
      "homepage": "https://github.blog/",
      "mode": "rss_config",
      "feed_url": "https://github.blog/feed/",
      "feed_status": "confirmed",
      "check_method": "configured",
      "status": "ok",
      "item_count": 10,
      "checked_at": "2026-02-22T02:52:45.428937+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "cloudflare-blog",
      "source_name": "Cloudflare Blog",
      "homepage": "https://blog.cloudflare.com/",
      "mode": "rss_config",
      "feed_url": "https://blog.cloudflare.com/rss/",
      "feed_status": "confirmed",
      "check_method": "configured",
      "status": "ok",
      "item_count": 20,
      "checked_at": "2026-02-22T02:52:45.988474+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "vercel-blog",
      "source_name": "Vercel Blog",
      "homepage": "https://vercel.com/blog",
      "mode": "rss_config",
      "feed_url": "https://vercel.com/atom",
      "feed_status": "confirmed",
      "check_method": "configured",
      "status": "ok",
      "item_count": 20,
      "checked_at": "2026-02-22T02:52:47.085625+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "google-research-blog",
      "source_name": "Google Research Blog",
      "homepage": "https://research.google/blog/",
      "mode": "rss_config",
      "feed_url": "https://research.google/blog/rss",
      "feed_status": "confirmed",
      "check_method": "configured",
      "status": "ok",
      "item_count": 20,
      "checked_at": "2026-02-22T02:52:50.225279+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "deepmind-blog",
      "source_name": "Google DeepMind Blog",
      "homepage": "https://deepmind.google/discover/blog/",
      "mode": "rss_config",
      "feed_url": "https://deepmind.google/blog/rss.xml",
      "feed_status": "confirmed",
      "check_method": "configured",
      "status": "ok",
      "item_count": 20,
      "checked_at": "2026-02-22T02:52:55.113658+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "meta-ai-blog",
      "source_name": "Meta AI Blog",
      "homepage": "https://ai.meta.com/blog/",
      "mode": "html",
      "feed_url": null,
      "feed_status": "not_found",
      "check_method": "http_probe",
      "status": "ok",
      "item_count": 13,
      "checked_at": "2026-02-22T02:53:05.647641+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "microsoft-foundry-blog",
      "source_name": "Microsoft Foundry Blog",
      "homepage": "https://devblogs.microsoft.com/foundry/",
      "mode": "rss_config",
      "feed_url": "https://devblogs.microsoft.com/foundry/feed/",
      "feed_status": "confirmed",
      "check_method": "configured",
      "status": "ok",
      "item_count": 10,
      "checked_at": "2026-02-22T02:53:07.192116+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "aws-ml-blog",
      "source_name": "AWS Machine Learning Blog",
      "homepage": "https://aws.amazon.com/blogs/machine-learning/",
      "mode": "rss_config",
      "feed_url": "https://aws.amazon.com/blogs/machine-learning/feed/",
      "feed_status": "confirmed",
      "check_method": "configured",
      "status": "ok",
      "item_count": 20,
      "checked_at": "2026-02-22T02:53:07.453358+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "mistral-news",
      "source_name": "Mistral News",
      "homepage": "https://mistral.ai/news",
      "mode": "html",
      "feed_url": null,
      "feed_status": "not_found",
      "check_method": "http_probe",
      "status": "ok",
      "item_count": 1,
      "checked_at": "2026-02-22T02:53:20.068035+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "cohere-blog",
      "source_name": "Cohere Blog",
      "homepage": "https://cohere.com/blog",
      "mode": "html",
      "feed_url": null,
      "feed_status": "not_found",
      "check_method": "http_probe",
      "status": "empty",
      "item_count": 0,
      "checked_at": "2026-02-22T02:53:28.608107+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "llamaindex-blog",
      "source_name": "LlamaIndex Blog",
      "homepage": "https://www.llamaindex.ai/blog",
      "mode": "html",
      "feed_url": null,
      "feed_status": "not_found",
      "check_method": "http_probe",
      "status": "ok",
      "item_count": 20,
      "checked_at": "2026-02-22T02:53:32.462209+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    },
    {
      "source_id": "crewai-blog",
      "source_name": "CrewAI Blog",
      "homepage": "https://www.crewai.com/blog",
      "mode": "html",
      "feed_url": null,
      "feed_status": "not_found",
      "check_method": "http_probe",
      "status": "ok",
      "item_count": 20,
      "checked_at": "2026-02-22T02:53:36.818459+00:00",
      "error": "",
      "retry_attempted": false,
      "retry_succeeded": false,
      "retry_count": 0,
      "skip_reason": "",
      "health_level": "healthy",
      "cooldown_until": null
    }
  ]
}